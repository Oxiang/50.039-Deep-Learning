{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "custom_dataset_dataloader_cascade.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moral-rebound"
      },
      "source": [
        "# Boilerplate notebook"
      ],
      "id": "moral-rebound"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crude-waste"
      },
      "source": [
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# Numpy\n",
        "import numpy as np\n",
        "# Pillow\n",
        "from PIL import Image\n",
        "# Torch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "# Misc\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "id": "crude-waste",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4gBSNDrcfGl"
      },
      "source": [
        "# 1. Download dataset"
      ],
      "id": "R4gBSNDrcfGl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcHbRtuHcd-N"
      },
      "source": [
        "!git clone -b data https://github.com/Oxiang/50.039-Deep-Learning.git"
      ],
      "id": "lcHbRtuHcd-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3ElMDwpU831"
      },
      "source": [
        "!sudo apt-get install tree"
      ],
      "id": "J3ElMDwpU831",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4-h2B3OVCRe"
      },
      "source": [
        "cd 50.039-Deep-Learning"
      ],
      "id": "g4-h2B3OVCRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW2ZD2afqdmF"
      },
      "source": [
        "%%bash\n",
        "\n",
        "(\n",
        "tree dataset -d\n",
        ") "
      ],
      "id": "DW2ZD2afqdmF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWQTRVbAVkPQ"
      },
      "source": [
        "# 2. Dataset high-level info\n",
        "\n",
        "The images stored in the **./dataset** folder and its subfolder consists of 150 by 150 pixels greyscale images, representing X-Ray pictures of lungs.\n",
        "\n",
        "We treat the problem as a cascading problem where we first predict normal vs infected then infected_non_covid vs infected_covid there will\n",
        "\n",
        "For normal vs infected\n",
        "\n",
        "| Description                | Class index | Tensor | Class label |\n",
        "| -------------------------- | ----------- | ------ | ----------- |\n",
        "| Normal                     | 0           | [1 0]  | normal      |\n",
        "| People with infected lungs | 1           | [0 1]  | infected    |\n",
        "\n",
        "For infected_covid vs infected_non_covid\n",
        "\n",
        "| Description                              | Class index | Tensor | Class label        |\n",
        "| ---------------------------------------- | ----------- | ------ | ------------------ |\n",
        "| People with infected lungs and covid     | 0           | [1 0]  | covid |\n",
        "| People with infected lungs and non-covid | 1           | [0 1]  | non_covid     |\n"
      ],
      "id": "aWQTRVbAVkPQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZhr10rHVlIj"
      },
      "source": [
        "classes_n_c = {0: 'normal', 1: 'infected'}\n",
        "classes_inc_ic = {0: 'infected_non_covid', 1: 'infected_covid'}\n",
        "groups = ['train', 'test', 'val']\n",
        "dataset_numbers = {\n",
        "    'train_normal': 1341,\n",
        "    'train_infected_non_covid': 2530,\n",
        "    'train_infected_covid': 1345,\n",
        "    'val_normal': 8,\n",
        "    'val_infected_non_covid': 8,\n",
        "    'val_infected_covid': 8,    \n",
        "    'test_normal': 234,\n",
        "    'test_infected_non_covid': 242,\n",
        "    'test_infected_covid': 138,\n",
        "}\n",
        "dataset_paths = {\n",
        "    'train_normal': './dataset/train/normal/',\n",
        "    'train_infected_non_covid': './dataset/train/infected/non-covid/',\n",
        "    'train_infected_covid': './dataset/train/infected/covid/',\n",
        "    'val_normal': './dataset/val/normal/',\n",
        "    'val_infected_non_covid': './dataset/val/infected/non-covid/',\n",
        "    'val_infected_covid': './dataset/val/infected/covid/',    \n",
        "    'test_normal': './dataset/test/normal/',\n",
        "    'test_infected_non_covid': './dataset/test/infected/non-covid/',\n",
        "    'test_infected_covid': './dataset/test/infected/covid/',    \n",
        "}"
      ],
      "id": "NZhr10rHVlIj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kakruEOWFn2"
      },
      "source": [
        "View one of the images and its properties. These images consist of a Numpy array, with values ranging between 0 and 255. These values will be normalized."
      ],
      "id": "_kakruEOWFn2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THPZFpETWAWK"
      },
      "source": [
        "path_to_file = './dataset/train/normal/1.jpg'\n",
        "with open(path_to_file, 'rb') as f:\n",
        "    im = np.asarray(Image.open(f))\n",
        "    plt.imshow(im)\n",
        "f.close()\n",
        "print('Image shape is: {}'.format(im.shape))\n",
        "# Images are defined as a Numpy array of values between 0 and 256\n",
        "print('Image as a numpy array is:\\n {}'.format(im))"
      ],
      "id": "THPZFpETWAWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcs9foiNXKY1"
      },
      "source": [
        "# 3. Creating a Dataset object"
      ],
      "id": "Jcs9foiNXKY1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyf61XksuJjY"
      },
      "source": [
        "## 3.1 Common variables"
      ],
      "id": "Pyf61XksuJjY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z0B8tOLuI-D"
      },
      "source": [
        "binary_dataset_paths = {\n",
        "    'layer_0': {\n",
        "        'train': {\n",
        "            'train_normal':'./dataset/train/normal',\n",
        "            'train_infected': './dataset/train/infected'\n",
        "        },\n",
        "        'val': {\n",
        "            'val_normal':'./dataset/val/normal',\n",
        "            'val_infected': './dataset/val/infected'\n",
        "        },\n",
        "        'test': {\n",
        "            'test_normal':'./dataset/test/normal',\n",
        "            'test_infected': './dataset/test/infected'\n",
        "        }\n",
        "    },\n",
        "    'layer_1':{\n",
        "        'train': {\n",
        "            'train_covid': './dataset/train/infected/covid',\n",
        "            'train_non_covid' : './dataset/train/infected/non-covid'\n",
        "        },\n",
        "        'val': {\n",
        "            'val_covid': './dataset/val/infected/covid',\n",
        "            'val_non_covid' : './dataset/val/infected/non-covid'\n",
        "        },\n",
        "        'test': {\n",
        "            'test_covid': './dataset/test/infected/covid',\n",
        "            'test_non_covid' : './dataset/test/infected/non-covid'            \n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "binary_dataset_numbers = {\n",
        "    'layer_0': {\n",
        "        'train': {\n",
        "            'train_normal': 1341,\n",
        "            'train_infected': 3875\n",
        "        },\n",
        "        'val': {\n",
        "            'val_normal': 8,\n",
        "            'val_infected': 16\n",
        "        },\n",
        "        'test': {\n",
        "            'test_normal': 234,\n",
        "            'test_infected': 380\n",
        "        }\n",
        "    },\n",
        "    'layer_1':{\n",
        "        'train': {\n",
        "            'train_covid': 1345,\n",
        "            'train_non_covid' : 2530\n",
        "        },\n",
        "        'val': {\n",
        "            'val_covid': 8,\n",
        "            'val_non_covid': 8\n",
        "        },\n",
        "        'test': {\n",
        "            'test_covid': 138,\n",
        "            'test_non_covid': 242            \n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "id": "1Z0B8tOLuI-D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKLCcIvxXLHu"
      },
      "source": [
        "## 3.2 Layer 0 General Dataset object that is custom made for train, val, test to individually use\n",
        "\n",
        "length method ( __ len __ )\n",
        "\n",
        "> return the number of images present in the dataset\n",
        "\n",
        "getitem method ( __ getitem __ )\n",
        "\n",
        "> fetch an image and its label, using a single index value. Returns the image, along with a one-hot vector corresponding to the class of the object. Both returned parameters will be torch tensors.\n",
        "- [1, 0] for normal class\n",
        "- [0, 1] for infected class"
      ],
      "id": "LKLCcIvxXLHu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtNXQwKbXNfY"
      },
      "source": [
        "class L0_Lung_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Generic Dataset class for Layer 0\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, groups, dataset_numbers, dataset_paths, infected_sub_class_numbers):\n",
        "        \"\"\"\n",
        "        Constructor for generic Dataset class for Layer0 - assembles\n",
        "        the important parameters in attributes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        groups : str\n",
        "            Allowed values: train, val, test\n",
        "        dataset_numbers : dict\n",
        "            Count of each class within specified group (e.g. normal, infected)\n",
        "        dataset_paths : dict\n",
        "            Path to each class within specified group (infected has 2 sub-class dir)\n",
        "        \"\"\"\n",
        "\n",
        "        self.img_size = (150, 150)\n",
        "        self.classes = { 0: 'normal', 1: 'infected' }\n",
        "        self.covid_status = {0: '', 1: 'covid', 2: 'non-covid'} \n",
        "        self.groups = groups\n",
        "        self.dataset_numbers = dataset_numbers\n",
        "        self.dataset_paths = dataset_paths\n",
        "        self.infected_sub_class_numbers = infected_sub_class_numbers\n",
        "\n",
        "        \n",
        "    def describe(self):\n",
        "        \"\"\"\n",
        "        Descriptor function.\n",
        "        Will print details about the dataset when called.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Generate description\n",
        "        msg = \"This is the {} dataset of the Lung Dataset\".format(self.groups)\n",
        "        msg += \" used for the Small Project Demo in the 50.039 Deep Learning class\"\n",
        "        msg += \" in March 2021. \\n\"\n",
        "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\n",
        "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
        "        msg += \"The images are stored in the following locations \"\n",
        "        msg += \"and each one contains the following number of images:\\n\"\n",
        "        for key, val in self.dataset_paths.items():\n",
        "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, val, self.dataset_numbers[key])\n",
        "        print(msg)\n",
        "        \n",
        "    \n",
        "    def open_img(self, group_val, class_val, covid_status, index_val):\n",
        "        \"\"\"\n",
        "        Opens image with specified parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\n",
        "        - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid'.\n",
        "        - covid_status should take values in '', 'covid' or 'non_covid'.\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
        "        \n",
        "        Returns loaded image as a normalized Numpy array.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Asserts checking for consistency in passed parameters\n",
        "        err_msg = \"Error - group_val variable should be set to 'train', 'test' or 'val'.\"\n",
        "        assert group_val in self.groups, err_msg\n",
        "        \n",
        "        err_msg = \"Error - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid.\"\n",
        "        assert class_val in self.classes.values(), err_msg\n",
        "\n",
        "        err_msg = \"Error - covid_status variable should be set to '', 'covid' or 'non-covid'.\"\n",
        "        assert covid_status in self.covid_status.values(), err_msg\n",
        "        \n",
        "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\n",
        "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
        "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val, max_val)\n",
        "        assert isinstance(index_val, int), err_msg\n",
        "        assert index_val >= 0 and index_val <= max_val, err_msg\n",
        "        \n",
        "        # 'normal' - example path: /dataset/train/normal/1.jpg\n",
        "        if covid_status == \"\":\n",
        "            path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\n",
        "        # 'covid' or 'non_covid' - example path: './dataset/train/infected/covid/1.jpg',\n",
        "        else:\n",
        "            path_to_file = '{}/{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], covid_status, index_val)\n",
        "\n",
        "        with open(path_to_file, 'rb') as f:\n",
        "            # Convert to Numpy array and normalize pixel values by dividing by 255.\n",
        "            im = np.asarray(Image.open(f))/255\n",
        "        f.close()\n",
        "        return im\n",
        "    \n",
        "    \n",
        "    def show_img(self, group_val, class_val, covid_status, index_val):\n",
        "        \"\"\"\n",
        "        Opens, then displays image with specified parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\n",
        "        - class_val variable should be set to 'normal' or 'infected'.\n",
        "        - covid_status should take values in '', 'covid' or 'non-covid'.\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Open image\n",
        "        im = self.open_img(group_val, class_val, covid_status, index_val)\n",
        "        \n",
        "        # Display\n",
        "        plt.imshow(im)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Length special method, returns the number of images in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Length function\n",
        "        return sum(self.dataset_numbers.values())\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Getitem special method.\n",
        "        \n",
        "        Expects an integer value index, between 0 and len(self) - 1.\n",
        "        \n",
        "        Returns the image and its label as a one hot vector, both\n",
        "        in torch tensor format in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Get item special method\n",
        "        first_val = int(list(self.dataset_numbers.values())[0])\n",
        "        if index < first_val:\n",
        "            class_val = 'normal'\n",
        "            label = torch.Tensor([1, 0])\n",
        "            covid_status = \"\"\n",
        "        else:\n",
        "            class_val = 'infected'\n",
        "            index = index - first_val\n",
        "            label = torch.Tensor([0, 1])\n",
        "            infected_covid_numbers = int(list(self.infected_sub_class_numbers.values())[0]) # covid\n",
        "            if index < infected_covid_numbers:\n",
        "                class_val = 'infected'\n",
        "                covid_status = 'covid'\n",
        "            else:\n",
        "                class_val = 'infected'\n",
        "                index = index - infected_covid_numbers\n",
        "                covid_status = 'non-covid'\n",
        "        im = self.open_img(self.groups, class_val, covid_status, index)\n",
        "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
        "        return im, label"
      ],
      "id": "LtNXQwKbXNfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V714D12V5qgY"
      },
      "source": [
        "def verify_l0_dataset(group,dataset,image_overall_index,class_val,covid_status,\n",
        "                   image_specific_dataset_index=1):\n",
        "  \"\"\"\n",
        "  Helper function to verify that the classes are implemented correctly\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  group : str\n",
        "      Allowed values: train, val, test\n",
        "  dataset: object\n",
        "      Object instantiated from the class\n",
        "  image_overall_index: int\n",
        "      Overall index in the full dataset across all classes\n",
        "  class_val: str\n",
        "      Image label. Example: normal, infected\n",
        "  covid_status: str\n",
        "      If class_val is 'infected', set this to either 'covid' or 'non-covid'\n",
        "  image_specific_dataset_index : int\n",
        "      image id in the specific nested directory\n",
        "  \"\"\"\n",
        "  print('Verify the special methods __len__ and __get_item__')\n",
        "  print('Number of images in {} dataset: {}'.format(group, len(dataset)))\n",
        "  print('Details for image id {} from the {} dataset'.format(\n",
        "      image_overall_index,\n",
        "      group\n",
        "  ))\n",
        "  im, class_oh = dataset[image_overall_index]\n",
        "  print('Sample image shape: {}'.format(im.shape))\n",
        "  print('Sample image: {}'.format(im))\n",
        "  print('Sample image class: {}'.format(class_oh))\n",
        "\n",
        "  print('\\nVerify the open_img and show_img functions')\n",
        "  print('Open and show image {} from the {}_{} dataset'.format(\n",
        "      image_specific_dataset_index,\n",
        "      group,\n",
        "      class_val\n",
        "  ))\n",
        "  im = dataset.open_img(group, class_val, covid_status, image_specific_dataset_index)\n",
        "  print('Sample image shape: {}'.format(im.shape))\n",
        "  print('Sample image: {}'.format(im))\n",
        "  dataset.show_img(group, class_val, covid_status, image_specific_dataset_index)"
      ],
      "id": "V714D12V5qgY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9CO29iBYHs-"
      },
      "source": [
        "### 3.2.1 Layer 0 Train dataset"
      ],
      "id": "s9CO29iBYHs-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBI4uwQqYiWI"
      },
      "source": [
        "train_group = 'train'\n",
        "l0_ld_train = L0_Lung_Dataset(groups = train_group,\n",
        "                              dataset_numbers = binary_dataset_numbers['layer_0'][train_group],\n",
        "                              dataset_paths = binary_dataset_paths['layer_0'][train_group],\n",
        "                              infected_sub_class_numbers = binary_dataset_numbers['layer_1'][train_group])\n",
        "l0_ld_train.describe()"
      ],
      "id": "LBI4uwQqYiWI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo4ZJTwpYjw4"
      },
      "source": [
        "verify_l0_dataset(train_group,l0_ld_train,1340,'normal','',1340)"
      ],
      "id": "jo4ZJTwpYjw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMz0d7zfY5W7"
      },
      "source": [
        "### 3.2.2 Layer 0 Validation dataset"
      ],
      "id": "YMz0d7zfY5W7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Sju4KFY5p8"
      },
      "source": [
        "val_group = 'val'\n",
        "l0_ld_val = L0_Lung_Dataset(groups = val_group,\n",
        "                            dataset_numbers = binary_dataset_numbers['layer_0'][val_group],\n",
        "                            dataset_paths = binary_dataset_paths['layer_0'][val_group],\n",
        "                            infected_sub_class_numbers = binary_dataset_numbers['layer_1'][val_group])\n",
        "l0_ld_val.describe()"
      ],
      "id": "w4Sju4KFY5p8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzhsBRE4Y_F_"
      },
      "source": [
        "verify_l0_dataset(val_group,l0_ld_val,9,'infected','covid',1)"
      ],
      "id": "PzhsBRE4Y_F_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojan7nynZILA"
      },
      "source": [
        "### 3.2.3 Layer 0 Test dataset"
      ],
      "id": "Ojan7nynZILA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdxD1NzmZIvq"
      },
      "source": [
        "test_group = 'test'\n",
        "l0_ld_test = L0_Lung_Dataset(groups = test_group, \n",
        "                              dataset_numbers = binary_dataset_numbers['layer_0'][test_group], \n",
        "                              dataset_paths = binary_dataset_paths['layer_0'][test_group],\n",
        "                              infected_sub_class_numbers = binary_dataset_numbers['layer_1'][test_group])\n",
        "l0_ld_test.describe()"
      ],
      "id": "XdxD1NzmZIvq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ohnFncZJJD"
      },
      "source": [
        "verify_l0_dataset(test_group,l0_ld_test,613,'infected','non-covid',241)"
      ],
      "id": "49ohnFncZJJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRmEfd3Z2ccn"
      },
      "source": [
        "## 3.3 Layer 1 General Dataset object that is custom made for train, val, test to individually use"
      ],
      "id": "QRmEfd3Z2ccn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWR5VOpK2pan"
      },
      "source": [
        "class L1_Lung_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Generic Dataset class for Layer 1\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, groups, dataset_numbers, dataset_paths):\n",
        "        \"\"\"\n",
        "        Constructor for generic Dataset class for Layer0 - assembles\n",
        "        the important parameters in attributes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        groups : str\n",
        "            Allowed values: train, val, test\n",
        "        dataset_numbers : dict\n",
        "            Count of each class within specified group (e.g. covid, non_covid)\n",
        "        dataset_paths : dict\n",
        "            Path to each class within specified group (infected has 2 sub-class dir)\n",
        "        \"\"\"\n",
        "\n",
        "        self.img_size = (150, 150)\n",
        "        self.classes = {0: 'covid', 1: 'non_covid'}\n",
        "        self.groups = groups\n",
        "        self.dataset_numbers = dataset_numbers\n",
        "        self.dataset_paths = dataset_paths\n",
        "\n",
        "        \n",
        "    def describe(self):\n",
        "        \"\"\"\n",
        "        Descriptor function.\n",
        "        Will print details about the dataset when called.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Generate description\n",
        "        msg = \"This is the {} dataset of the Lung Dataset\".format(self.groups)\n",
        "        msg += \" used for the Small Project Demo in the 50.039 Deep Learning class\"\n",
        "        msg += \" in March 2021. \\n\"\n",
        "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\n",
        "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
        "        msg += \"The images are stored in the following locations \"\n",
        "        msg += \"and each one contains the following number of images:\\n\"\n",
        "        for key, val in self.dataset_paths.items():\n",
        "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, val, self.dataset_numbers[key])\n",
        "        print(msg)\n",
        "        \n",
        "    \n",
        "    def open_img(self, group_val, class_val, index_val):\n",
        "        \"\"\"\n",
        "        Opens image with specified parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\n",
        "        - class_val variable should be set to 'covid' or 'non-covid'.\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
        "        \n",
        "        Returns loaded image as a normalized Numpy array.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Asserts checking for consistency in passed parameters\n",
        "        err_msg = \"Error - group_val variable should be set to 'train', 'test' or 'val'.\"\n",
        "        assert group_val in self.groups, err_msg\n",
        "        \n",
        "        err_msg = \"Error - class_val variable should be set to 'covid' or 'non-covid'.\"\n",
        "        assert class_val in self.classes.values(), err_msg     \n",
        "        \n",
        "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\n",
        "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
        "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val, max_val)\n",
        "        assert isinstance(index_val, int), err_msg\n",
        "        assert index_val >= 0 and index_val <= max_val, err_msg\n",
        "        \n",
        "        # Open file as before\n",
        "        path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\n",
        "        with open(path_to_file, 'rb') as f:\n",
        "            im = np.asarray(Image.open(f))/255\n",
        "        f.close()\n",
        "        return im\n",
        "    \n",
        "    \n",
        "    def show_img(self, group_val, class_val, index_val):\n",
        "        \"\"\"\n",
        "        Opens, then displays image with specified parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\n",
        "        - class_val variable should be set to 'covid' or 'non-covid'.\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Open image\n",
        "        im = self.open_img(group_val, class_val, index_val)\n",
        "        \n",
        "        # Display\n",
        "        plt.imshow(im)\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Length special method, returns the number of images in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Length function\n",
        "        return sum(self.dataset_numbers.values())\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Getitem special method.\n",
        "        \n",
        "        Expects an integer value index, between 0 and len(self) - 1.\n",
        "        \n",
        "        Returns the image and its label as a one hot vector, both\n",
        "        in torch tensor format in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Get item special method\n",
        "        first_val = int(list(self.dataset_numbers.values())[0])\n",
        "        if index < first_val:\n",
        "            class_val = 'covid'\n",
        "            label = torch.Tensor([1, 0])\n",
        "        else:\n",
        "            class_val = 'non_covid'\n",
        "            index = index - first_val\n",
        "            label = torch.Tensor([0, 1])\n",
        "\n",
        "        im = self.open_img(self.groups, class_val, index)\n",
        "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
        "        return im, label"
      ],
      "id": "lWR5VOpK2pan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB6wqXUN5wMd"
      },
      "source": [
        "def verify_l1_dataset(group,dataset,image_overall_index,class_val,\n",
        "                   image_specific_dataset_index=1):\n",
        "  \"\"\"\n",
        "  Helper function to verify that the classes are implemented correctly\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  group : str\n",
        "      Allowed values: train, val, test\n",
        "  dataset: object\n",
        "      Object instantiated from the class\n",
        "  image_overall_index: int\n",
        "      Overall index in the full dataset across all classes\n",
        "  class_val: str\n",
        "      Image label. Example: covid, non-covid\n",
        "  image_specific_dataset_index : int\n",
        "      image id in the specific nested directory\n",
        "  \"\"\"\n",
        "  print('Verify the special methods __len__ and __get_item__')\n",
        "  print('Number of images in {} dataset: {}'.format(group, len(dataset)))\n",
        "  print('Details for image id {} from the {} dataset'.format(\n",
        "      image_overall_index,\n",
        "      group\n",
        "  ))\n",
        "  im, class_oh = dataset[image_overall_index]\n",
        "  print('Sample image shape: {}'.format(im.shape))\n",
        "  print('Sample image: {}'.format(im))\n",
        "  print('Sample image class: {}'.format(class_oh))\n",
        "\n",
        "  print('\\nVerify the open_img and show_img functions')\n",
        "  print('Open and show image {} from the {}_{} dataset'.format(\n",
        "      image_specific_dataset_index,\n",
        "      group,\n",
        "      class_val\n",
        "  ))\n",
        "  im = dataset.open_img(group, class_val, image_specific_dataset_index)\n",
        "  print('Sample image shape: {}'.format(im.shape))\n",
        "  print('Sample image: {}'.format(im))\n",
        "  dataset.show_img(group, class_val, image_specific_dataset_index)"
      ],
      "id": "MB6wqXUN5wMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIeOkWaC2n3U"
      },
      "source": [
        "### 3.2.1 Layer 1 Train dataset"
      ],
      "id": "vIeOkWaC2n3U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McMnU4Jm6X58"
      },
      "source": [
        "train_group = 'train'\n",
        "l1_ld_train = L1_Lung_Dataset(groups = train_group, \n",
        "                              dataset_numbers = binary_dataset_numbers['layer_1'][train_group], \n",
        "                              dataset_paths = binary_dataset_paths['layer_1'][train_group])\n",
        "l1_ld_train.describe()"
      ],
      "id": "McMnU4Jm6X58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1lsv9yE6Y3g"
      },
      "source": [
        "verify_l1_dataset(train_group,l1_ld_train,1,'covid',1)"
      ],
      "id": "Z1lsv9yE6Y3g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZt-Y09H2gcy"
      },
      "source": [
        "### 3.2.2 Layer 1 Validation dataset"
      ],
      "id": "TZt-Y09H2gcy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP8YEmM48GAs"
      },
      "source": [
        "val_group = 'val'\n",
        "l1_ld_val = L1_Lung_Dataset(groups = val_group, \n",
        "                              dataset_numbers = binary_dataset_numbers['layer_1'][val_group], \n",
        "                              dataset_paths = binary_dataset_paths['layer_1'][val_group])\n",
        "l1_ld_val.describe()"
      ],
      "id": "UP8YEmM48GAs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnTb6R989Kc-"
      },
      "source": [
        "verify_l1_dataset(val_group,l1_ld_val,15,'non_covid',7)"
      ],
      "id": "qnTb6R989Kc-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSZCUuuS6RLL"
      },
      "source": [
        "### 3.2.3 Layer 1 Test dataset"
      ],
      "id": "CSZCUuuS6RLL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx5IOpZN8Gws"
      },
      "source": [
        "test_group = 'test'\n",
        "l1_ld_test = L1_Lung_Dataset(groups = test_group, \n",
        "                              dataset_numbers = binary_dataset_numbers['layer_1'][test_group], \n",
        "                              dataset_paths = binary_dataset_paths['layer_1'][test_group])\n",
        "l1_ld_test.describe()"
      ],
      "id": "cx5IOpZN8Gws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-5moiDN9fAJ"
      },
      "source": [
        "verify_l1_dataset(test_group,l1_ld_test,379,'non_covid',241)"
      ],
      "id": "R-5moiDN9fAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XwbFL6DbSxg"
      },
      "source": [
        "# 4. Data visualization\n",
        "\n",
        "This requires a `grouped bar chart`. Refer to [matplotlib Grouped bar chart with labels](https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html) for starter code"
      ],
      "id": "0XwbFL6DbSxg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6BuWE6vDGMW"
      },
      "source": [
        "def plot_grouped_bar_chart(labels, train, val, test):\n",
        "  '''\n",
        "  Plots a grouped bar chart\n",
        "  '''\n",
        "  x = np.arange(len(labels))  # the label locations\n",
        "  width = 0.25  # the width of the bars\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  rects1 = ax.bar(x - width, train, width, label='train')\n",
        "  rects2 = ax.bar(x + width, val, width, label='val')\n",
        "  rects3 = ax.bar(x, test, width, label='test')\n",
        "\n",
        "  ax.set_ylabel('Number of datapoints')\n",
        "  ax.set_title('Number of datapoints with respect to each dataset and class')\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.legend()\n",
        "\n",
        "  def autolabel(rects):\n",
        "      \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "      for rect in rects:\n",
        "          height = rect.get_height()\n",
        "          ax.annotate('{}'.format(height),\n",
        "                      xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                      xytext=(0, 3),  # 3 points vertical offset\n",
        "                      textcoords=\"offset points\",\n",
        "                      ha='center', va='bottom')\n",
        "\n",
        "  autolabel(rects1)\n",
        "  autolabel(rects2)\n",
        "  autolabel(rects3)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "id": "V6BuWE6vDGMW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAt9MHTVDGaw"
      },
      "source": [
        "## 4.1 Overall\n",
        "\n",
        "<u>**Discuss whether or not the dataset is balanced between classes, uniformly distributed, etc.**</u>\n",
        "\n",
        "**Training set**\n",
        "\n",
        "The train data for the different classes are imbalanced. From the graph plotted below, the `infected_non_covid` class has significantly more data points than the other classes. Overall, the ratio `normal:infected_non_covid:infected_covid` is approximately `1:2:1`.\n",
        "\n",
        "This could present more complications if the model is trained in a stacking manner by first training normal vs infected. The ratio of `normal:infected` would be a ratio of `1:3` which is more imbalanced. However, if there are distinct differences in the data of the normal and infected then this large difference could be negated. Alternatively, data augmentation techniques could be used to increase the size of the training data for the `normal` class.\n",
        "\n",
        "**Testing set**\n",
        "\n",
        "The test set is also slightly imbalanced with the ratio `normal:infected_non_covid:infected_covid` being approximately `2:2:1`. However, this is not as bad as an imbalanced training set because the test set will not only be used to get a gauge of the model's performance after each epoch and will not directly affect the model's parameter tuning.\n",
        "\n",
        "**Validation set**\n",
        "\n",
        "The val set is uniformly distributed between the three classes. However, it is glaring that there are only 8 validation samples for the 3 classes. However, this is acceptable as it is used only for validation and will not affect the model's parameter tuning"
      ],
      "id": "kAt9MHTVDGaw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7ySGoEpAFwl"
      },
      "source": [
        "labels = ['normal', 'infected_covid', 'infected_non_covid']\n",
        "\n",
        "train_normal = [l0_ld_train.dataset_numbers['train_normal']]\n",
        "val_normal = [l0_ld_val.dataset_numbers['val_normal']]\n",
        "test_normal = [l0_ld_test.dataset_numbers['test_normal']]\n",
        "\n",
        "train_covid_non_covid = list(l1_ld_train.dataset_numbers.values())\n",
        "val_covid_non_covid = list(l1_ld_val.dataset_numbers.values())\n",
        "test_covid_non_covid = list(l1_ld_test.dataset_numbers.values())\n",
        "\n",
        "train_normal_inc_ic = train_normal + train_covid_non_covid\n",
        "val_normal_inc_ic = val_normal + val_covid_non_covid\n",
        "test_normal_inc_ic = test_normal + test_covid_non_covid\n",
        "\n",
        "plot_grouped_bar_chart(labels, train_normal_inc_ic, val_normal_inc_ic, test_normal_inc_ic)"
      ],
      "id": "V7ySGoEpAFwl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YfpYP7lDwWe"
      },
      "source": [
        "## 4.2 Layer 0\n",
        "\n",
        "The graph below gives a Clearer idea about the training imbalance where the ratio of `normal:infected` would be a ratio of `1:3` which are the ratios used when training the binary classifier for Layer 0."
      ],
      "id": "_YfpYP7lDwWe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-DIZ_jPbVFh"
      },
      "source": [
        "labels = ['normal', 'infected']\n",
        "\n",
        "train_normal_infected = list(l0_ld_train.dataset_numbers.values())\n",
        "val_normal_infected = list(l0_ld_val.dataset_numbers.values())\n",
        "test_normal_infected = list(l0_ld_test.dataset_numbers.values())\n",
        "\n",
        "plot_grouped_bar_chart(labels, train_normal_infected, val_normal_infected, test_normal_infected)"
      ],
      "id": "x-DIZ_jPbVFh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7BThBfXEZmU"
      },
      "source": [
        "## 4.3 Layer 1\n",
        "\n",
        "The class distribution for infected_covid vs infected_non_covid is imbalanced by about 1:2. This represents what is likely in reality. Covid is new and will probably not have as many data samples as the common infected non-covid X rays. Again, possible data augmentations techniques like mirroring could potentially be used to increase the number of training samples for covid"
      ],
      "id": "l7BThBfXEZmU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO2nrsCI_gt9"
      },
      "source": [
        "labels = ['covid', 'non-covid']\n",
        "\n",
        "train_covid_non_covid = list(l1_ld_train.dataset_numbers.values())\n",
        "val_covid_non_covid = list(l1_ld_val.dataset_numbers.values())\n",
        "test_covid_non_covid = list(l1_ld_test.dataset_numbers.values())\n",
        "\n",
        "plot_grouped_bar_chart(labels, train_covid_non_covid, val_covid_non_covid, test_covid_non_covid)"
      ],
      "id": "OO2nrsCI_gt9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFl4rsh5p1rX"
      },
      "source": [
        "# 4. Why normalize the data?\n",
        "\n",
        "To recap the normalization can be found in the `Lung_Dataset` class' `open_img` function which did the following normalization.\n",
        "\n",
        "```python\n",
        "# Convert to Numpy array and normalize pixel values by dividing by 255.\n",
        "im = np.asarray(Image.open(f))/255\n",
        "```\n",
        "\n",
        "Images have RGB ranges from 0-255. Considering various activation functions like `sigmoid` such a large range would mean that for vastly different values like 100 and 255, not much difference can be seen when passed into the `sigmoid` activation function. Both would produce a value that is close to 1.\n",
        "\n",
        "Taking the same values as reference, if we divide by 255, for a value of 100,  $\\frac{100}{255}$ we get approximately 0.39. Then for a value of 255, $\\frac{255}{255}$ we get 1. For the initial value of 100 that becomes 0.39 after the division, passing it into `sigmoid(0.39)` produces a value of 0.596. Meanwhile for the initial value of 255 that becomes 1 after division, passing it into `sigmoid(1)` produces a value of 0.731. This difference in value allows us to extract meaningful differences in the pixel values.\n"
      ],
      "id": "BFl4rsh5p1rX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6jfWGpZqCIU"
      },
      "source": [
        "# 5. Other possible pre-processing\n",
        "\n",
        "Form the plot below, which is based on the Training set for normal images as reference, it is evident that there are several differences in the photo dimensions and photo environment. \n",
        "\n",
        "For example, comparing image_index 1 and image_index 28 there is a clear difference in the lighting, Image_index 28 is a lot brighter. One pre-processing step could be to use histogram normalization. There is a paper that recommends 14 possible normalization algorithms that can be performed (Leszczynski, 2010)\n",
        "\n",
        "Aother example is comparing \"skinny\" images like image_index 1 and image_index 31 where there is significantly more dark backgrounds at the side compares to images like image_index 12. Perhaps a edge detection algorithm can be applied to just filter the relevant parts of the image which are the lungs."
      ],
      "id": "d6jfWGpZqCIU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgyLkZ9xp2GH"
      },
      "source": [
        "row = 2\n",
        "col = 2\n",
        "\n",
        "selected_indices = [1,12,28,31]\n",
        "f, axarr = plt.subplots(row,col,figsize=(10,7))\n",
        "counter = 0\n",
        "for row_index in range(row):\n",
        "  for col_index in range(col):\n",
        "    image_index = selected_indices[counter]\n",
        "    im = l0_ld_train.open_img('train', 'normal', '', image_index)\n",
        "    axarr[row_index,col_index].set_title('Image index: {}'.format(image_index))\n",
        "    axarr[row_index,col_index].imshow(im)\n",
        "    counter += 1"
      ],
      "id": "lgyLkZ9xp2GH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC14yqAz2vSI"
      },
      "source": [
        "# 6. Creating a data loader object"
      ],
      "id": "wC14yqAz2vSI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5YY_qNrFUTw"
      },
      "source": [
        "## 6.1 Layer 0"
      ],
      "id": "J5YY_qNrFUTw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiftrqf62usb"
      },
      "source": [
        "l0_bs_train = 32\n",
        "l0_bs_test = 32\n",
        "l0_bs_val = 1\n",
        "l0_train_loader = DataLoader(l0_ld_train, batch_size = l0_bs_train, shuffle = True)\n",
        "l0_test_loader = DataLoader(l0_ld_test, batch_size = l0_bs_test, shuffle = True)\n",
        "l0_val_loader = DataLoader(l0_ld_val, batch_size = l0_bs_val, shuffle = True)"
      ],
      "id": "uiftrqf62usb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Fv6HyVFsfa"
      },
      "source": [
        "## 6.2 Layer 1"
      ],
      "id": "B4Fv6HyVFsfa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpHtBlbSFTZW"
      },
      "source": [
        "l1_bs_train = 32\n",
        "l1_bs_test = 32\n",
        "l1_bs_val = 1\n",
        "l1_train_loader = DataLoader(l1_ld_train, batch_size = l1_bs_train, shuffle = True)\n",
        "l1_test_loader = DataLoader(l1_ld_test, batch_size = l1_bs_test, shuffle = True)\n",
        "l1_val_loader = DataLoader(l1_ld_val, batch_size = l1_bs_val, shuffle = True)"
      ],
      "id": "wpHtBlbSFTZW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x_-1Qe34U5w"
      },
      "source": [
        "# 7. Model\n",
        "\n"
      ],
      "id": "3x_-1Qe34U5w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVfcawPjF5sl"
      },
      "source": [
        "## 7.1 Layer 0"
      ],
      "id": "BVfcawPjF5sl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yJ7JTk04XIu"
      },
      "source": [
        "class L0_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(L0_Net, self).__init__()\n",
        "        # Conv2D: 1 input channel, 4 output channels, 3 by 3 kernel, stride of 1.\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
        "        self.fc1 = nn.Linear(87616, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        output = F.log_softmax(x, dim = 1)\n",
        "        return output"
      ],
      "id": "-yJ7JTk04XIu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJKx7kXTQYQN"
      },
      "source": [
        "# Activate gpu\n",
        "if torch.cuda.is_available():  \n",
        "    print('using GPU')\n",
        "    device = \"cuda:0\" \n",
        "else:  \n",
        "    device = \"cpu\"\n",
        "l0_model = L0_Net().to(torch.device(device))"
      ],
      "id": "EJKx7kXTQYQN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbl4-OO417"
      },
      "source": [
        "summary(l0_model, (1, 150, 150))"
      ],
      "id": "zoBbl4-OO417",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUtueamzGHp9"
      },
      "source": [
        "## 7.2 Layer 1"
      ],
      "id": "UUtueamzGHp9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El3FLR03GJ5m"
      },
      "source": [
        "class L1_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(L1_Net, self).__init__()\n",
        "        # Conv2D: 1 input channel, 4 output channels, 3 by 3 kernel, stride of 1.\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
        "        self.fc1 = nn.Linear(87616, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        output = F.log_softmax(x, dim = 1)\n",
        "        return output"
      ],
      "id": "El3FLR03GJ5m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3uSnZgeGKO-"
      },
      "source": [
        "# Activate gpu\n",
        "if torch.cuda.is_available():  \n",
        "    print('using GPU')\n",
        "    device = \"cuda:0\" \n",
        "else:  \n",
        "    device = \"cpu\"\n",
        "l1_model = L1_Net().to(torch.device(device))"
      ],
      "id": "o3uSnZgeGKO-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tRV7PJIGPTH"
      },
      "source": [
        "summary(l1_model, (1, 150, 150))"
      ],
      "id": "2tRV7PJIGPTH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E4MDvTLdnP"
      },
      "source": [
        "# 8. Training the model\n",
        "\n",
        "Reference material: [Towards data science: PyTorch [Tabular]  Multiclass Classification](https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab)"
      ],
      "id": "L1E4MDvTLdnP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98hO2OLpLdRK"
      },
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "    correct_pred = (y_pred_tags == y_test).float()\n",
        "    acc = correct_pred.sum() / len(correct_pred)\n",
        "    \n",
        "    return acc"
      ],
      "id": "98hO2OLpLdRK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsnZg2ToGTfx"
      },
      "source": [
        "## 8.1 Layer 0"
      ],
      "id": "IsnZg2ToGTfx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7yVj4RlPNNV"
      },
      "source": [
        "# Define criterion and optimizer, epoch\n",
        "epochs = 3\n",
        "lr = 0.0001\n",
        "# Normal : Infected\n",
        "l0_class_weights = torch.tensor([2.8, 1.0]).to(torch.device(device))\n",
        "criterion = nn.CrossEntropyLoss(l0_class_weights)\n",
        "optimizer = optim.Adam(l0_model.parameters(), lr = lr)"
      ],
      "id": "E7yVj4RlPNNV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJxZ00RSPTBy"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "start_model_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "model_name = 'binary_L0_net'\n",
        "path = './checkpoint_model_{}_{}_{}'.format(epochs, model_name, start_model_time)\n",
        "\n",
        "l0_accuracy_stats_epoch = {\n",
        "    'train': [],\n",
        "    'test': [],\n",
        "    'epoch': [],\n",
        "}\n",
        "l0_loss_stats_epoch = {\n",
        "    'train': [],\n",
        "    'test': [],\n",
        "    'epoch': [],\n",
        "}\n",
        "\n",
        "for e in range(epochs):\n",
        "    train_epoch_loss = 0\n",
        "    train_epoch_acc = 0\n",
        "    \n",
        "    l0_model.train()\n",
        "    for X_train_batch, y_train_batch in l0_train_loader:\n",
        "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = l0_model.forward(X_train_batch)\n",
        "        train_loss  = criterion(output, torch.max(y_train_batch, 1)[1])\n",
        "        train_acc = multi_acc(output, torch.max(y_train_batch, 1)[1])\n",
        "        \n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_epoch_loss += train_loss.item()\n",
        "        train_epoch_acc += train_acc.item()\n",
        "\n",
        "    # testing\n",
        "    with torch.no_grad():\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_acc = 0\n",
        "        l0_model.eval()\n",
        "        for X_test_batch, y_test_batch in l0_test_loader:\n",
        "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "\n",
        "            y_test_pred = l0_model.forward(X_test_batch)\n",
        "\n",
        "            test_loss = criterion(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
        "            test_acc = multi_acc(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
        "\n",
        "            test_epoch_loss += test_loss.item()\n",
        "            test_epoch_acc += test_acc.item()\n",
        "            \n",
        "    \n",
        "    # averaged\n",
        "    train_epoch_loss = train_epoch_loss/len(l0_train_loader)\n",
        "    train_epoch_acc = train_epoch_acc/len(l0_train_loader)\n",
        "    test_epoch_loss = test_epoch_loss/len(l0_test_loader)\n",
        "    test_epoch_acc = test_epoch_acc/len(l0_test_loader)\n",
        "    \n",
        "    # The step number corresponds to the number of batches seen\n",
        "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    print(\"Epoch: {}/{} - {} - \".format(e+1, epochs, now),\n",
        "      \"Training Loss: {:.4f} - \".format(train_epoch_loss),\n",
        "      \"Training Accuracy: {:.4f}\".format(train_epoch_acc),\n",
        "      \"Test Loss: {:.4f} - \".format(test_epoch_loss),\n",
        "      \"Test Accuracy: {:.4f}\".format(test_epoch_acc))\n",
        "    l0_model.train()\n",
        "    \n",
        "    # Epoch metrics\n",
        "    l0_loss_stats_epoch['train'].append(train_epoch_loss)\n",
        "    l0_loss_stats_epoch['test'].append(test_epoch_loss)\n",
        "    l0_loss_stats_epoch['epoch'].append(e+1)\n",
        "    l0_accuracy_stats_epoch['train'].append(train_epoch_acc)\n",
        "    l0_accuracy_stats_epoch['test'].append(test_epoch_acc)\n",
        "    l0_accuracy_stats_epoch['epoch'].append(e+1)\n",
        "    \n",
        "    # auto-save a checkpoint at every epoch for evaluation and continual training. Overrides the previous checkpoint\n",
        "    checkpoint = {\n",
        "        'epochs': e+1,\n",
        "        'model_state_dict': l0_model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_epoch_loss,\n",
        "        'test_loss': test_epoch_loss,\n",
        "        'test_accuracy': test_epoch_acc,\n",
        "        'training_accuracy': train_epoch_acc\n",
        "    }\n",
        "    torch.save(checkpoint, path)"
      ],
      "id": "lJxZ00RSPTBy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRYDiwdsL4vB"
      },
      "source": [
        "## 8.2 Layer 1"
      ],
      "id": "WRYDiwdsL4vB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSctEJ0tL49-"
      },
      "source": [
        "# Define criterion and optimizer, epoch\n",
        "epochs = 3\n",
        "lr = 0.0001\n",
        "l1_class_weights = torch.tensor([1.5, 1]).to(torch.device(device))\n",
        "criterion = nn.CrossEntropyLoss(l1_class_weights)\n",
        "optimizer = optim.Adam(l1_model.parameters(), lr = lr)"
      ],
      "id": "bSctEJ0tL49-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p-7XimOMBVF"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "start_model_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "model_name = 'binary_L1_net'\n",
        "path = './checkpoint_model_{}_{}_{}'.format(epochs, model_name, start_model_time)\n",
        "\n",
        "l1_accuracy_stats_epoch = {\n",
        "    'train': [],\n",
        "    'test': [],\n",
        "    'epoch': [],\n",
        "}\n",
        "l1_loss_stats_epoch = {\n",
        "    'train': [],\n",
        "    'test': [],\n",
        "    'epoch': [],\n",
        "}\n",
        "\n",
        "for e in range(epochs):\n",
        "    train_epoch_loss = 0\n",
        "    train_epoch_acc = 0\n",
        "    \n",
        "    l1_model.train()\n",
        "    for X_train_batch, y_train_batch in l1_train_loader:\n",
        "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = l1_model.forward(X_train_batch)\n",
        "        train_loss  = criterion(output, torch.max(y_train_batch, 1)[1])\n",
        "        train_acc = multi_acc(output, torch.max(y_train_batch, 1)[1])\n",
        "        \n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_epoch_loss += train_loss.item()\n",
        "        train_epoch_acc += train_acc.item()\n",
        "\n",
        "    # testing\n",
        "    with torch.no_grad():\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_acc = 0\n",
        "        l1_model.eval()\n",
        "        for X_test_batch, y_test_batch in l1_test_loader:\n",
        "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "\n",
        "            y_test_pred = l1_model.forward(X_test_batch)\n",
        "\n",
        "            test_loss = criterion(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
        "            test_acc = multi_acc(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
        "\n",
        "            test_epoch_loss += test_loss.item()\n",
        "            test_epoch_acc += test_acc.item()\n",
        "            \n",
        "    \n",
        "    # averaged\n",
        "    train_epoch_loss = train_epoch_loss/len(l1_train_loader)\n",
        "    train_epoch_acc = train_epoch_acc/len(l1_train_loader)\n",
        "    test_epoch_loss = test_epoch_loss/len(l1_test_loader)\n",
        "    test_epoch_acc = test_epoch_acc/len(l1_test_loader)\n",
        "    \n",
        "    # The step number corresponds to the number of batches seen\n",
        "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    print(\"Epoch: {}/{} - {} - \".format(e+1, epochs, now),\n",
        "      \"Training Loss: {:.4f} - \".format(train_epoch_loss),\n",
        "      \"Training Accuracy: {:.4f}\".format(train_epoch_acc),\n",
        "      \"Test Loss: {:.4f} - \".format(test_epoch_loss),\n",
        "      \"Test Accuracy: {:.4f}\".format(test_epoch_acc))\n",
        "    l1_model.train()\n",
        "    \n",
        "    # Epoch metrics\n",
        "    l1_loss_stats_epoch['train'].append(train_epoch_loss)\n",
        "    l1_loss_stats_epoch['test'].append(test_epoch_loss)\n",
        "    l1_loss_stats_epoch['epoch'].append(e+1)\n",
        "    l1_accuracy_stats_epoch['train'].append(train_epoch_acc)\n",
        "    l1_accuracy_stats_epoch['test'].append(test_epoch_acc)\n",
        "    l1_accuracy_stats_epoch['epoch'].append(e+1)\n",
        "    \n",
        "    # auto-save a checkpoint at every epoch for evaluation and continual training. Overrides the previous checkpoint\n",
        "    checkpoint = {\n",
        "        'epochs': e+1,\n",
        "        'model_state_dict': l1_model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_epoch_loss,\n",
        "        'test_loss': test_epoch_loss, \n",
        "        'test_accuracy': test_epoch_acc,\n",
        "        'training_accuracy': train_epoch_acc\n",
        "    }\n",
        "    torch.save(checkpoint, path)"
      ],
      "id": "-p-7XimOMBVF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUG2wrXboT8T"
      },
      "source": [
        "# 9. Graph visualizations of learning curves\n",
        "\n",
        "Graphs covered\n",
        "\n",
        "1. Accuracy against steps\n",
        "2. Recall against steps\n",
        "3. Precision against steps\n",
        "4. F1 against steps\n",
        "5. Loss against steps"
      ],
      "id": "cUG2wrXboT8T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhUVlMGJoW0N"
      },
      "source": [
        "def plot_graph(data,datasets,y_axis,x_axis):\n",
        "  \"\"\"Plots a graph of y against x\n",
        "\n",
        "    Keyword arguments:\n",
        "    data -- dictionary with keys: train, val, epoch, steps\n",
        "    datasets -- list of datasets to plot: train, val\n",
        "    y_axis -- name of the y_axis. example: accuracy\n",
        "    x_axis -- either epoch or steps\n",
        "    \"\"\"\n",
        "  plt.figure(figsize=(17,8))\n",
        "  for dataset in datasets:\n",
        "    target_y = data[dataset]\n",
        "    target_x = data[x_axis]\n",
        "    plt.plot(target_x, target_y, label = dataset)\n",
        "  plt.xlabel(x_axis)\n",
        "  plt.ylabel(y_axis)\n",
        "  plt.title('Graph of {} against {}'.format(y_axis, x_axis))\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "id": "JhUVlMGJoW0N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZqZ2V9FMIsL"
      },
      "source": [
        "## 9.1 Layer 0"
      ],
      "id": "OZqZ2V9FMIsL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWJD3RFbcyxd"
      },
      "source": [
        "plot_graph(l0_accuracy_stats_epoch, ['train', 'test'], 'accuracy', 'epoch')\n",
        "plot_graph(l0_loss_stats_epoch, ['train', 'test'], 'loss', 'epoch')"
      ],
      "id": "UWJD3RFbcyxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpBmQLzAMLmO"
      },
      "source": [
        "## 9.2 Layer 1"
      ],
      "id": "jpBmQLzAMLmO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFDVrasJMNEp"
      },
      "source": [
        "plot_graph(l1_accuracy_stats_epoch, ['train', 'test'], 'accuracy', 'epoch')\n",
        "plot_graph(l1_loss_stats_epoch, ['train', 'test'], 'loss', 'epoch')"
      ],
      "id": "SFDVrasJMNEp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGpqG8Y6es_7"
      },
      "source": [
        "# 10. Validation set performance"
      ],
      "id": "cGpqG8Y6es_7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbbJvYKcNIkd"
      },
      "source": [
        "## 10.1 Init dataset and data loader for 3 classes"
      ],
      "id": "vbbJvYKcNIkd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhLZ0u26NRB0"
      },
      "source": [
        "class Lung_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Generic Dataset class.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, groups, dataset_numbers, dataset_paths):\n",
        "        \"\"\"\n",
        "        Constructor for generic Dataset class - assembles\n",
        "        the important parameters in attributes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        groups : str\n",
        "            Allowed values: train, val, test\n",
        "        dataset_numbers : dict\n",
        "            Count of each class within specified group\n",
        "        dataset_paths : dict\n",
        "            Path to each class within specified group\n",
        "        \"\"\"\n",
        "\n",
        "        self.img_size = (150, 150)\n",
        "        self.classes = {\n",
        "            0: 'normal',\n",
        "            1: 'infected_covid',\n",
        "            2: 'infected_non_covid'\n",
        "        }        \n",
        "        self.groups = groups\n",
        "        self.dataset_numbers = dataset_numbers\n",
        "        self.dataset_paths = dataset_paths\n",
        "        \n",
        "        \n",
        "    def describe(self):\n",
        "        \"\"\"\n",
        "        Descriptor function.\n",
        "        Will print details about the dataset when called.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Generate description\n",
        "        msg = \"This is the {} dataset of the Lung Dataset\".format(self.groups)\n",
        "        msg += \" used for the Small Project Demo in the 50.039 Deep Learning class\"\n",
        "        msg += \" in March 2021. \\n\"\n",
        "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\n",
        "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
        "        msg += \"The images are stored in the following locations \"\n",
        "        msg += \"and each one contains the following number of images:\\n\"\n",
        "        for key, val in self.dataset_paths.items():\n",
        "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, val, self.dataset_numbers[key])\n",
        "        print(msg)\n",
        "        \n",
        "    \n",
        "    def open_img(self, group_val, class_val, index_val):\n",
        "        \"\"\"\n",
        "        Opens image with specified parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\n",
        "        - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid'.\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
        "        \n",
        "        Returns loaded image as a normalized Numpy array.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Asserts checking for consistency in passed parameters\n",
        "        err_msg = \"Error - group_val variable should be set to 'train', 'test' or 'val'.\"\n",
        "        assert group_val in self.groups, err_msg\n",
        "        \n",
        "        err_msg = \"Error - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid.\"\n",
        "        assert class_val in self.classes.values(), err_msg\n",
        "        \n",
        "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\n",
        "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
        "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val, max_val)\n",
        "        assert isinstance(index_val, int), err_msg\n",
        "        assert index_val >= 0 and index_val <= max_val, err_msg\n",
        "        \n",
        "        # Open file as before\n",
        "        path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\n",
        "        with open(path_to_file, 'rb') as f:\n",
        "            # Convert to Numpy array and normalize pixel values by dividing by 255.\n",
        "            im = np.asarray(Image.open(f))/255\n",
        "        f.close()\n",
        "        return im\n",
        "    \n",
        "    \n",
        "    def show_img(self, group_val, class_val, index_val):\n",
        "        \"\"\"\n",
        "        Opens, then displays image with specified parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\n",
        "        - class_val variable should be set to 'normal' or 'infected_covid' or 'infected_non_covid'\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Open image\n",
        "        im = self.open_img(group_val, class_val, index_val)\n",
        "        \n",
        "        # Display\n",
        "        plt.imshow(im)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Length special method, returns the number of images in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Length function\n",
        "        return sum(self.dataset_numbers.values())\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Getitem special method.\n",
        "        \n",
        "        Expects an integer value index, between 0 and len(self) - 1.\n",
        "        \n",
        "        Returns the image and its label as a one hot vector, both\n",
        "        in torch tensor format in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Get item special method\n",
        "        first_val = int(list(self.dataset_numbers.values())[0])\n",
        "        second_val = int(list(self.dataset_numbers.values())[1])\n",
        "        if index < first_val:\n",
        "            class_val = 'normal'\n",
        "            label = torch.Tensor([1, 0, 0])\n",
        "        elif index < (first_val+second_val):\n",
        "            class_val = 'infected_covid'\n",
        "            index = index - first_val\n",
        "            label = torch.Tensor([0, 1, 0])\n",
        "        else:\n",
        "            class_val = \"infected_non_covid\"\n",
        "            index = index - (first_val+second_val)\n",
        "            label = torch.Tensor([0, 0, 1])\n",
        "        im = self.open_img(self.groups, class_val, index)\n",
        "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
        "        return im, label"
      ],
      "id": "HhLZ0u26NRB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmF7BnwUNdxF"
      },
      "source": [
        "dataset_numbers_val = {\n",
        "    'val_normal': 8,\n",
        "    'val_infected_covid': 8,\n",
        "    'val_infected_non_covid': 8,\n",
        "}\n",
        "dataset_paths_val = {\n",
        "    'val_normal': './dataset/val/normal/',\n",
        "    'val_infected_covid': './dataset/val/infected/covid/',\n",
        "    'val_infected_non_covid': './dataset/val/infected/non-covid/',\n",
        "}\n",
        "val_group = 'val'\n",
        "ld_val = Lung_Dataset(\n",
        "    val_group,\n",
        "    dataset_numbers_val,\n",
        "    dataset_paths_val\n",
        ")\n",
        "ld_val.describe()"
      ],
      "id": "OmF7BnwUNdxF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSgP9-kDNOtu"
      },
      "source": [
        "## 10.2 Perform validation"
      ],
      "id": "kSgP9-kDNOtu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ku1pIryfPga"
      },
      "source": [
        "def combinePred(l0_model, l1_model, val_batch):\n",
        "    '''\n",
        "    val_batch will be batch_size of 1 from\n",
        "\n",
        "    Keyword arguments:\n",
        "    l0_model -- L0_Net object\n",
        "    l1_model -- L1_Net object\n",
        "    val_batch -- DataLoader object\n",
        "    '''\n",
        "    l0_model.eval()\n",
        "    l0_pred = l0_model.forward(val_batch.to(device))\n",
        "    l0_pred = int(torch.max(l0_pred, 1)[1])\n",
        "    if l0_pred == 0:\n",
        "        # normal\n",
        "        return torch.Tensor([1, 0, 0])\n",
        "    else:\n",
        "        l1_model.eval()\n",
        "        l1_pred = l1_model.forward(val_batch.to(device))\n",
        "        l1_pred = int(torch.max(l1_pred, 1)[1])\n",
        "        if l1_pred == 0:\n",
        "            # covid\n",
        "            return torch.Tensor([0, 1, 0])\n",
        "        else:\n",
        "            # non-covid\n",
        "            return torch.Tensor([0, 0, 1])\n",
        "    \n",
        "\n",
        "def get_ground_truth_prediction(data_loader):\n",
        "    '''Gets the predicted & ground truth labels for each image in data_loader\n",
        "\n",
        "    Keyword arguments:\n",
        "    data_loader -- DataLoader object\n",
        "    '''\n",
        "    correct = 0\n",
        "    result = {\n",
        "        'im': [],\n",
        "        'ground_truth': [],\n",
        "        'prediction': [],\n",
        "        'index': []\n",
        "    }\n",
        "    for batch_idx, (X_val_batch, ground_truth) in enumerate(data_loader):\n",
        "        ground_truth = torch.max(ground_truth,1)[1].item()\n",
        "        ground_truth = ld_val.classes.get(ground_truth)\n",
        "        prediction = combinePred(l0_model, l1_model, X_val_batch)\n",
        "        prediction = torch.max(prediction,0)[1].item()\n",
        "        prediction = ld_val.classes.get(prediction)\n",
        "        if ground_truth == prediction:\n",
        "          correct += 1\n",
        "        im, class_oh = ld_val[batch_idx]\n",
        "        result['im'].append(im)\n",
        "        result['ground_truth'].append(ground_truth)\n",
        "        result['prediction'].append(prediction)\n",
        "        result['index'].append(batch_idx)\n",
        "    return result, correct\n",
        "\n",
        "def plot_ground_truth_prediction(row, col, result):\n",
        "    '''Plots the subplots of images with their ground truth and prediction\n",
        "\n",
        "    Keyword arguments:\n",
        "    row -- int value for number of rows of plot\n",
        "    col -- int value for number of columns of plot\n",
        "    result -- dict containing index, prediction, ground truth, image\n",
        "    '''\n",
        "    f, axarr = plt.subplots(row,col,figsize=(20,30))\n",
        "    f.suptitle('Validation set pictures, with predicted and ground truth labels.\\nAverage performance {}/{}={}%'.format(correct,total,accuracy))\n",
        "    counter = 0\n",
        "    for row_index in range(row):\n",
        "        for col_index in range(col):\n",
        "            axarr[row_index,col_index].set_title(\n",
        "                'Ground truth label: {}\\nPredicted label: {}'.format(\n",
        "                    result['ground_truth'][counter],\n",
        "                    result['prediction'][counter])\n",
        "                )\n",
        "            axarr[row_index,col_index].imshow(torch.squeeze(result['im'][counter]))\n",
        "            counter += 1\n",
        "\n",
        "val_loader = DataLoader(ld_val, batch_size = 1, shuffle = False)\n",
        "total = len(val_loader)\n",
        "result, correct = get_ground_truth_prediction(val_loader)\n",
        "accuracy = round(correct/total,3) * 100\n",
        "row = 6\n",
        "col = 4\n",
        "\n",
        "plot_ground_truth_prediction(row, col, result)"
      ],
      "id": "8Ku1pIryfPga",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZTINHs2qdMC"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "val_loader = DataLoader(ld_val, batch_size = 1, shuffle = False)\n",
        "\n",
        "acc = 0\n",
        "# row = truth, col = pred\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for X_val_batch, y_val_batch in val_loader:\n",
        "    X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "    \n",
        "    y_val_pred = combinePred(l0_model, l1_model, X_val_batch)\n",
        "    y_val_pred = y_val_pred.to(device)\n",
        "    batch_acc = int(torch.argmax(y_val_batch)) == int(torch.argmax(y_val_pred))\n",
        "    y_true.append(int(torch.argmax(y_val_batch)))\n",
        "    y_pred.append(int(torch.argmax(y_val_pred)))\n",
        "    acc += batch_acc\n",
        "    \n",
        "print('combined accuracy:', acc/len(val_loader))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.array(['normal','covid','non-covid']))\n",
        "disp.plot()"
      ],
      "id": "eZTINHs2qdMC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP-BWTPBxk-s"
      },
      "source": [
        "# 11. Other useful metrics\n",
        "\n",
        "Precision = $\\frac{tp}{tp + fp}$\n",
        "\n",
        "Recall = $\\frac{tp}{tp + fn}$\n",
        "\n",
        "For this project, the metrics can be applied differently considering the needs of different stakeholders.\n",
        "\n",
        "**Considering Covid, we cite 2 stakeholders who could have different concerns**\n",
        "\n",
        "For example, the general public could be concerned about the spread of the virus, hence, they could be more worried about Recall because false negatives could mean that people with the virus in the public space and mingling with others.\n",
        "\n",
        "On the other hand, the clinicans could be more concerned about Precision. This is because there are limited hospital beds and it would be a waste of resources to allocated scarce resource to people who do not have covid but were predicted to have covid."
      ],
      "id": "MP-BWTPBxk-s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVcqyL_jtr1G"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import json\n",
        "\n",
        "p_r_fs = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0, 1, 2])\n",
        "precision = p_r_fs[0]\n",
        "recall = p_r_fs[1]\n",
        "f_beta_score = p_r_fs[2]\n",
        "\n",
        "overall_results = {}\n",
        "\n",
        "overall_labels = ['normal','covid','non-covid']\n",
        "\n",
        "for index, label in enumerate(overall_labels):\n",
        "  overall_results[label] = {}\n",
        "  overall_results[label]['precision'] = precision[index]\n",
        "  overall_results[label]['recall'] = recall[index]\n",
        "  overall_results[label]['f_beta_score'] = f_beta_score[index]\n",
        "\n",
        "print(json.dumps(overall_results, indent=4, sort_keys=True))"
      ],
      "id": "FVcqyL_jtr1G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uULHf8Q-3sEQ"
      },
      "source": [
        "# 12. Feature maps\n",
        "\n",
        "Reference: https://discuss.pytorch.org/t/visualize-feature-map/29597"
      ],
      "id": "uULHf8Q-3sEQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n-LT-Ub3uBw"
      },
      "source": [
        "# Visualize feature maps\n",
        "activation = {}\n",
        "# f, axarr = plt.subplots(row,col,figsize=(20,30))\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "for batch_idx, (X_val_batch, ground_truth) in enumerate(val_loader):\n",
        "    if batch_idx == 1:\n",
        "        break\n",
        "    l0_model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "    output = l0_model(X_val_batch.to(device))\n",
        "    act = activation['conv1'].squeeze()\n",
        "\n",
        "fig, axarr = plt.subplots(len(val_loader),act.size(0),figsize=(28,60))\n",
        "for batch_idx, (X_val_batch, ground_truth) in enumerate(val_loader):\n",
        "    l0_model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "    output = l0_model(X_val_batch.to(device))\n",
        "    act = activation['conv1'].squeeze()\n",
        "    mid_point = act.size(0) / 2\n",
        "    for idx in range(act.size(0)):\n",
        "        if idx == mid_point:\n",
        "            axarr[batch_idx,idx].set_title(\n",
        "                      'Ground Truth for Row {}: {}\\nPrediction for row {}: {}'.format(\n",
        "                          batch_idx,\n",
        "                          result['ground_truth'][batch_idx],\n",
        "                          batch_idx,\n",
        "                          result['prediction'][batch_idx])\n",
        "                      )\n",
        "        else:\n",
        "            axarr[batch_idx,idx].set_title('')\n",
        "        axarr[batch_idx, idx].imshow(act[idx].cpu().numpy())"
      ],
      "id": "5n-LT-Ub3uBw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAYa04jUA1X8"
      },
      "source": [
        "ld_val.show_img('val', 'normal', 0)"
      ],
      "id": "TAYa04jUA1X8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k01aNmdgxVX-"
      },
      "source": [
        "# References\n",
        "\n",
        "Leszczynski, M. (2010). Image Preprocessing for Illumination Invariant Face \n",
        "Verification. Journal of telecommunications and information technology, 19-25."
      ],
      "id": "k01aNmdgxVX-"
    }
  ]
}