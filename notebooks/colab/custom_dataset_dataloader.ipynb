{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "custom_dataset_dataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moral-rebound"
      },
      "source": [
        "# Boilerplate notebook"
      ],
      "id": "moral-rebound"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crude-waste"
      },
      "source": [
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# Numpy\n",
        "import numpy as np\n",
        "# Pillow\n",
        "from PIL import Image\n",
        "# Torch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "# Misc\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "id": "crude-waste",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4gBSNDrcfGl"
      },
      "source": [
        "# 1. Download dataset"
      ],
      "id": "R4gBSNDrcfGl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcHbRtuHcd-N"
      },
      "source": [
        "!git clone -b data https://github.com/Oxiang/50.039-Deep-Learning.git"
      ],
      "id": "lcHbRtuHcd-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3ElMDwpU831"
      },
      "source": [
        "!sudo apt-get install tree"
      ],
      "id": "J3ElMDwpU831",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4-h2B3OVCRe"
      },
      "source": [
        "cd 50.039-Deep-Learning"
      ],
      "id": "g4-h2B3OVCRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW2ZD2afqdmF"
      },
      "source": [
        "%%bash\r\n",
        "\r\n",
        "(\r\n",
        "tree dataset -d\r\n",
        ") "
      ],
      "id": "DW2ZD2afqdmF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWQTRVbAVkPQ"
      },
      "source": [
        "# 2. Dataset high-level info\r\n",
        "\r\n",
        "The images stored in the **./dataset** folder and its subfolder consists of 150 by 150 pixels greyscale images, representing X-Ray pictures of lungs.\r\n",
        "\r\n",
        "The images, consists of X-ray pictures of the following:\r\n",
        "\r\n",
        "| Description                              | Class index | Tensor  | Class label        |\r\n",
        "| ---------------------------------------- | ----------- | ------- | ------------------ |\r\n",
        "| People with no infection diagnosis       | 0           | [1 0 0] | normal             |\r\n",
        "| People with infected lungs and non-covid | 1           | [0 1 0] | infected_non_covid |\r\n",
        "| People with infected lungs and covid     | 2           | [0 0 1] | infected_covid     |\r\n",
        "\r\n"
      ],
      "id": "aWQTRVbAVkPQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZhr10rHVlIj"
      },
      "source": [
        "classes = {0: 'normal', 1: 'infected_non_covid', 2: 'infected_covid'}\r\n",
        "groups = ['train', 'test', 'val']\r\n",
        "dataset_numbers = {\r\n",
        "    'train_normal': 1341,\r\n",
        "    'train_infected_non_covid': 2530,\r\n",
        "    'train_infected_covid': 1345,\r\n",
        "    'val_normal': 8,\r\n",
        "    'val_infected_non_covid': 8,\r\n",
        "    'val_infected_covid': 8,    \r\n",
        "    'test_normal': 234,\r\n",
        "    'test_infected_non_covid': 242,\r\n",
        "    'test_infected_covid': 138,\r\n",
        "}\r\n",
        "dataset_paths = {\r\n",
        "    'train_normal': './dataset/train/normal/',\r\n",
        "    'train_infected_non_covid': './dataset/train/infected/non-covid/',\r\n",
        "    'train_infected_covid': './dataset/train/infected/covid/',\r\n",
        "    'val_normal': './dataset/val/normal/',\r\n",
        "    'val_infected_non_covid': './dataset/val/infected/non-covid/',\r\n",
        "    'val_infected_covid': './dataset/val/infected/covid/',    \r\n",
        "    'test_normal': './dataset/test/normal/',\r\n",
        "    'test_infected_non_covid': './dataset/test/infected/non-covid/',\r\n",
        "    'test_infected_covid': './dataset/test/infected/covid/',    \r\n",
        "}"
      ],
      "id": "NZhr10rHVlIj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kakruEOWFn2"
      },
      "source": [
        "View one of the images and its properties. These images consist of a Numpy array, with values ranging between 0 and 255. These values will be normalized."
      ],
      "id": "_kakruEOWFn2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THPZFpETWAWK"
      },
      "source": [
        "path_to_file = './dataset/train/normal/1.jpg'\r\n",
        "with open(path_to_file, 'rb') as f:\r\n",
        "    im = np.asarray(Image.open(f))\r\n",
        "    plt.imshow(im)\r\n",
        "f.close()\r\n",
        "print('Image shape is: {}'.format(im.shape))\r\n",
        "# Images are defined as a Numpy array of values between 0 and 256\r\n",
        "print('Image as a numpy array is:\\n {}'.format(im))"
      ],
      "id": "THPZFpETWAWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcs9foiNXKY1"
      },
      "source": [
        "# 3. Creating a Dataset object"
      ],
      "id": "Jcs9foiNXKY1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKLCcIvxXLHu"
      },
      "source": [
        "## 3.1 General Dataset object that is custom made for train, val, test to individually use\r\n",
        "\r\n",
        "length method ( __ len __ )\r\n",
        "\r\n",
        "> return the number of images present in the dataset\r\n",
        "\r\n",
        "getitem method ( __ getitem __ )\r\n",
        "\r\n",
        "> fetch an image and its label, using a single index value. Returns the image, along with a one-hot vector corresponding to the class of the object. Both returned parameters will be torch tensors.\r\n",
        "- [1, 0,0] for normal class\r\n",
        "- [0, 1, 0] for infected_non_covid class\r\n",
        "- [0, 0, 1] for infected_covid class"
      ],
      "id": "LKLCcIvxXLHu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtNXQwKbXNfY"
      },
      "source": [
        "class Lung_Dataset(Dataset):\r\n",
        "    \"\"\"\r\n",
        "    Generic Dataset class.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    def __init__(self, groups, dataset_numbers, dataset_paths):\r\n",
        "        \"\"\"\r\n",
        "        Constructor for generic Dataset class - assembles\r\n",
        "        the important parameters in attributes.\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        groups : str\r\n",
        "            Allowed values: train, val, test\r\n",
        "        dataset_numbers : dict\r\n",
        "            Count of each class within specified group\r\n",
        "        dataset_paths : dict\r\n",
        "            Path to each class within specified group\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        self.img_size = (150, 150)\r\n",
        "        self.classes = {\r\n",
        "            0: 'normal',\r\n",
        "            1: 'infected_non_covid',\r\n",
        "            2: 'infected_covid'\r\n",
        "        }        \r\n",
        "        self.groups = groups\r\n",
        "        self.dataset_numbers = dataset_numbers\r\n",
        "        self.dataset_paths = dataset_paths\r\n",
        "        \r\n",
        "        \r\n",
        "    def describe(self):\r\n",
        "        \"\"\"\r\n",
        "        Descriptor function.\r\n",
        "        Will print details about the dataset when called.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # Generate description\r\n",
        "        msg = \"This is the {} dataset of the Lung Dataset\".format(self.groups)\r\n",
        "        msg += \" used for the Small Project Demo in the 50.039 Deep Learning class\"\r\n",
        "        msg += \" in March 2021. \\n\"\r\n",
        "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\r\n",
        "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\r\n",
        "        msg += \"The images are stored in the following locations \"\r\n",
        "        msg += \"and each one contains the following number of images:\\n\"\r\n",
        "        for key, val in self.dataset_paths.items():\r\n",
        "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, val, self.dataset_numbers[key])\r\n",
        "        print(msg)\r\n",
        "        \r\n",
        "    \r\n",
        "    def open_img(self, group_val, class_val, index_val):\r\n",
        "        \"\"\"\r\n",
        "        Opens image with specified parameters.\r\n",
        "        \r\n",
        "        Parameters:\r\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\r\n",
        "        - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid'.\r\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\r\n",
        "        \r\n",
        "        Returns loaded image as a normalized Numpy array.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # Asserts checking for consistency in passed parameters\r\n",
        "        err_msg = \"Error - group_val variable should be set to 'train', 'test' or 'val'.\"\r\n",
        "        assert group_val in self.groups, err_msg\r\n",
        "        \r\n",
        "        err_msg = \"Error - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid.\"\r\n",
        "        assert class_val in self.classes.values(), err_msg\r\n",
        "        \r\n",
        "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\r\n",
        "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\r\n",
        "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val, max_val)\r\n",
        "        assert isinstance(index_val, int), err_msg\r\n",
        "        assert index_val >= 0 and index_val <= max_val, err_msg\r\n",
        "        \r\n",
        "        # Open file as before\r\n",
        "        path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\r\n",
        "        with open(path_to_file, 'rb') as f:\r\n",
        "            # Convert to Numpy array and normalize pixel values by dividing by 255.\r\n",
        "            im = np.asarray(Image.open(f))/255\r\n",
        "        f.close()\r\n",
        "        return im\r\n",
        "    \r\n",
        "    \r\n",
        "    def show_img(self, group_val, class_val, index_val):\r\n",
        "        \"\"\"\r\n",
        "        Opens, then displays image with specified parameters.\r\n",
        "        \r\n",
        "        Parameters:\r\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\r\n",
        "        - class_val variable should be set to 'normal' or 'infected'.\r\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # Open image\r\n",
        "        im = self.open_img(group_val, class_val, index_val)\r\n",
        "        \r\n",
        "        # Display\r\n",
        "        plt.imshow(im)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        \"\"\"\r\n",
        "        Length special method, returns the number of images in dataset.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # Length function\r\n",
        "        return sum(self.dataset_numbers.values())\r\n",
        "    \r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"\r\n",
        "        Getitem special method.\r\n",
        "        \r\n",
        "        Expects an integer value index, between 0 and len(self) - 1.\r\n",
        "        \r\n",
        "        Returns the image and its label as a one hot vector, both\r\n",
        "        in torch tensor format in dataset.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        # Get item special method\r\n",
        "        first_val = int(list(self.dataset_numbers.values())[0])\r\n",
        "        second_val = int(list(self.dataset_numbers.values())[1])\r\n",
        "        if index < first_val:\r\n",
        "            class_val = 'normal'\r\n",
        "            label = torch.Tensor([1, 0, 0])\r\n",
        "        elif index < (first_val+second_val):\r\n",
        "            class_val = 'infected_non_covid'\r\n",
        "            index = index - first_val\r\n",
        "            label = torch.Tensor([0, 1, 0])\r\n",
        "        else:\r\n",
        "            class_val = \"infected_covid\"\r\n",
        "            index = index - (first_val+second_val)\r\n",
        "            label = torch.Tensor([0, 0, 1])\r\n",
        "        im = self.open_img(self.groups, class_val, index)\r\n",
        "        im = transforms.functional.to_tensor(np.array(im)).float()\r\n",
        "        return im, label"
      ],
      "id": "LtNXQwKbXNfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOYB7aYX9O-"
      },
      "source": [
        "dataset_numbers = {\r\n",
        "    'train': {\r\n",
        "        'train_normal': 1341,\r\n",
        "        'train_infected_non_covid': 2530,\r\n",
        "        'train_infected_covid': 1345,\r\n",
        "    },\r\n",
        "    'val': {\r\n",
        "        'val_normal': 8,\r\n",
        "        'val_infected_non_covid': 8,\r\n",
        "        'val_infected_covid': 8,\r\n",
        "    },\r\n",
        "    'test': {\r\n",
        "        'test_normal': 234,\r\n",
        "        'test_infected_non_covid': 242,\r\n",
        "        'test_infected_covid': 138,\r\n",
        "    }\r\n",
        "}\r\n",
        "dataset_paths = {\r\n",
        "    'train': {\r\n",
        "        'train_normal': './dataset/train/normal/',\r\n",
        "        'train_infected_non_covid': './dataset/train/infected/non-covid/',\r\n",
        "        'train_infected_covid': './dataset/train/infected/covid/',\r\n",
        "    },\r\n",
        "    'val': {\r\n",
        "        'val_normal': './dataset/val/normal/',\r\n",
        "        'val_infected_non_covid': './dataset/val/infected/non-covid/',\r\n",
        "        'val_infected_covid': './dataset/val/infected/covid/',\r\n",
        "    },\r\n",
        "    'test': {\r\n",
        "        'test_normal': './dataset/test/normal/',\r\n",
        "        'test_infected_non_covid': './dataset/test/infected/non-covid/',\r\n",
        "        'test_infected_covid': './dataset/test/infected/covid/',\r\n",
        "    }\r\n",
        "}"
      ],
      "id": "dcOYB7aYX9O-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS4ivSxsYKgl"
      },
      "source": [
        "def verify_dataset(group,dataset,image_overall_index=7,class_val='normal',\r\n",
        "                   image_specific_dataset_index=1):\r\n",
        "  print('Verify the special methods __len__ and __get_item__')\r\n",
        "  print('Number of images in {} dataset: {}'.format(group, len(dataset)))\r\n",
        "  print('Details for image id {} from the {} dataset'.format(\r\n",
        "      image_overall_index,\r\n",
        "      group\r\n",
        "  ))\r\n",
        "  im, class_oh = dataset[image_overall_index]\r\n",
        "  print('Sample image shape: {}'.format(im.shape))\r\n",
        "  print('Sample image: {}'.format(im))\r\n",
        "  print('Sample image class: {}'.format(class_oh))\r\n",
        "\r\n",
        "  print('\\nVerify the open_img and show_img functions')\r\n",
        "  print('Open and show image {} from the {}_{} dataset'.format(\r\n",
        "      image_specific_dataset_index,\r\n",
        "      group,\r\n",
        "      class_val\r\n",
        "  ))\r\n",
        "  im = dataset.open_img(group, class_val, image_specific_dataset_index)\r\n",
        "  print('Same sample image shape: {}'.format(im.shape))\r\n",
        "  print('Same sample image: {}'.format(im))\r\n",
        "  dataset.show_img(group, class_val, image_specific_dataset_index)"
      ],
      "id": "lS4ivSxsYKgl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9CO29iBYHs-"
      },
      "source": [
        "## 3.2 Train dataset"
      ],
      "id": "s9CO29iBYHs-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBI4uwQqYiWI"
      },
      "source": [
        "train_group = 'train'\r\n",
        "ld_train = Lung_Dataset(\r\n",
        "    train_group,\r\n",
        "    dataset_numbers[train_group],\r\n",
        "    dataset_paths[train_group]\r\n",
        ")\r\n",
        "ld_train.describe()"
      ],
      "id": "LBI4uwQqYiWI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo4ZJTwpYjw4"
      },
      "source": [
        "verify_dataset(train_group,ld_train,1)"
      ],
      "id": "jo4ZJTwpYjw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMz0d7zfY5W7"
      },
      "source": [
        "## 3.3 Validation dataset"
      ],
      "id": "YMz0d7zfY5W7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Sju4KFY5p8"
      },
      "source": [
        "val_group = 'val'\r\n",
        "ld_val = Lung_Dataset(\r\n",
        "    val_group,\r\n",
        "    dataset_numbers[val_group],\r\n",
        "    dataset_paths[val_group]\r\n",
        ")\r\n",
        "ld_val.describe()"
      ],
      "id": "w4Sju4KFY5p8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzhsBRE4Y_F_"
      },
      "source": [
        "verify_dataset(val_group,ld_val,1)"
      ],
      "id": "PzhsBRE4Y_F_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojan7nynZILA"
      },
      "source": [
        "## 3.4 Test dataset"
      ],
      "id": "Ojan7nynZILA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdxD1NzmZIvq"
      },
      "source": [
        "test_group = 'test'\r\n",
        "ld_test = Lung_Dataset(\r\n",
        "    test_group,\r\n",
        "    dataset_numbers[test_group],\r\n",
        "    dataset_paths[test_group]\r\n",
        ")\r\n",
        "ld_test.describe()"
      ],
      "id": "XdxD1NzmZIvq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ohnFncZJJD"
      },
      "source": [
        "verify_dataset(test_group,ld_test,1)"
      ],
      "id": "49ohnFncZJJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XwbFL6DbSxg"
      },
      "source": [
        "# 4. Data visualization\r\n",
        "\r\n",
        "This requires a `grouped bar chart`. Refer to [matplotlib Grouped bar chart with labels](https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html) for starter code\r\n",
        "\r\n",
        "<u>**Discuss whether or not the dataset is balanced between classes, uniformly distributed, etc.**</u>\r\n",
        "\r\n",
        "**Training set**\r\n",
        "\r\n",
        "The train data for the different classes are imbalanced. From the graph plotted below, the `infected_non_covid` class has significantly more data points than the other classes. Overall, the ratio `normal:infected_non_covid:infected_covid` is approximately `1:2:1`.\r\n",
        "\r\n",
        "This could present more complications if the model is trained in a stacking manner by first training normal vs infected. The ratio of `normal:infected` would be a ratio of `1:3` which is more imbalanced.\r\n",
        "\r\n",
        "**Testing set**\r\n",
        "\r\n",
        "The test set is also slightly imbalanced with the ratio `normal:infected_non_covid:infected_covid` being approximately `2:2:1`. However, this is not as bad as an imbalanced training set because the test set will not affect the model's parameter tuning.\r\n",
        "\r\n",
        "**Validation set**\r\n",
        "\r\n",
        "The val set is uniformly distributed between the three classes. However, it is glaring that there are only 8 validation samples for the 3 classes. Considering the ratio of `train:val:test`, the number of validation samples is far too low. For example. with reference to the infected_non_covid class, the ratio of `train:val:test` is `316:1:30`. which is quite far off from the recommended ratios like `80:10:10` or `8:1:1` as indicated by [Stanford's CS230](https://cs230.stanford.edu/blog/split/)."
      ],
      "id": "0XwbFL6DbSxg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-DIZ_jPbVFh"
      },
      "source": [
        "labels = ['normal', 'infected_non_covid', 'infected_covid']\r\n",
        "\r\n",
        "train_normal_inc_ic = list(ld_train.dataset_numbers.values())\r\n",
        "val_normal_inc_ic = list(ld_val.dataset_numbers.values())\r\n",
        "test_normal_inc_ic = list(ld_test.dataset_numbers.values())\r\n",
        "\r\n",
        "x = np.arange(len(labels))  # the label locations\r\n",
        "width = 0.25  # the width of the bars\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(10,10))\r\n",
        "rects1 = ax.bar(x - width, train_normal_inc_ic, width, label='train')\r\n",
        "rects2 = ax.bar(x + width, val_normal_inc_ic, width, label='val')\r\n",
        "rects3 = ax.bar(x, test_normal_inc_ic, width, label='test')\r\n",
        "\r\n",
        "ax.set_ylabel('Number of datapoints')\r\n",
        "ax.set_title('Number of datapoints with respect to each dataset and class')\r\n",
        "ax.set_xticks(x)\r\n",
        "ax.set_xticklabels(labels)\r\n",
        "ax.legend()\r\n",
        "\r\n",
        "def autolabel(rects):\r\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\r\n",
        "    for rect in rects:\r\n",
        "        height = rect.get_height()\r\n",
        "        ax.annotate('{}'.format(height),\r\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\r\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\r\n",
        "                    textcoords=\"offset points\",\r\n",
        "                    ha='center', va='bottom')\r\n",
        "\r\n",
        "autolabel(rects1)\r\n",
        "autolabel(rects2)\r\n",
        "autolabel(rects3)\r\n",
        "\r\n",
        "fig.tight_layout()\r\n",
        "plt.show()"
      ],
      "id": "x-DIZ_jPbVFh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFl4rsh5p1rX"
      },
      "source": [
        "# 4. Why normalize the data?\r\n",
        "\r\n",
        "To recap the normalization can be found in the `Lung_Dataset` class' `open_img` function which did the following normalization.\r\n",
        "\r\n",
        "```python\r\n",
        "# Convert to Numpy array and normalize pixel values by dividing by 255.\r\n",
        "im = np.asarray(Image.open(f))/255\r\n",
        "```\r\n",
        "\r\n",
        "Images have RGB ranges from 0-255. Considering various activation functions like `sigmoid` such a large range would mean that for vastly different values like 100 and 255, not much difference can be seen when passed into the `sigmoid` activation function. Both would produce a value that is close to 1.\r\n",
        "\r\n",
        "Taking the same values as reference, if we divide by 255, for a value of 100,  $\\frac{100}{255}$ we get approximately 0.39. Then for a value of 255, $\\frac{255}{255}$ we get 1. For the initial value of 100 that becomes 0.39 after the division, passing it into `sigmoid(0.39)` produces a value of 0.596. Meanwhile for the initial value of 255 that becomes 1 after division, passing it into `sigmoid(1)` produces a value of 0.731. This difference in value allows us to extract meaningful differences in the pixel values.\r\n"
      ],
      "id": "BFl4rsh5p1rX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6jfWGpZqCIU"
      },
      "source": [
        "# 5. Other possible pre-processing\r\n",
        "\r\n",
        "Form the plot below, which is based on the Training set for normal images as reference, it is evident that there are several differences in the photo dimensions and photo environment. \r\n",
        "\r\n",
        "For example, comparing image_index 1 and image_index 28 there is a clear difference in the lighting, Image_index 28 is a lot brighter. One pre-processing step could be to use histogram normalization. There is a paper that recommends 14 possible normalization algorithms that can be performed (Leszczynski, 2010)\r\n",
        "\r\n",
        "Aother example is comparing \"skinny\" images like image_index 1 and image_index 31 where there is significantly more dark backgrounds at the side compares to images like image_index 12. Perhaps a edge detection algorithm can be applied to just filter the relevant parts of the image which are the lungs."
      ],
      "id": "d6jfWGpZqCIU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgyLkZ9xp2GH"
      },
      "source": [
        "row = 2\r\n",
        "col = 2\r\n",
        "\r\n",
        "selected_indices = [1,12,28,31]\r\n",
        "f, axarr = plt.subplots(row,col,figsize=(10,7))\r\n",
        "counter = 0\r\n",
        "for row_index in range(row):\r\n",
        "  for col_index in range(col):\r\n",
        "    image_index = selected_indices[counter]\r\n",
        "    im = ld_train.open_img('train', 'normal', image_index)\r\n",
        "    axarr[row_index,col_index].set_title('Image index: {}'.format(image_index))\r\n",
        "    axarr[row_index,col_index].imshow(im)\r\n",
        "    counter += 1"
      ],
      "id": "lgyLkZ9xp2GH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC14yqAz2vSI"
      },
      "source": [
        "# 6. Creating a data loader object"
      ],
      "id": "wC14yqAz2vSI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiftrqf62usb"
      },
      "source": [
        "bs_val = 4\r\n",
        "train_loader = DataLoader(ld_train, batch_size = bs_val, shuffle = True)\r\n",
        "test_loader = DataLoader(ld_test, batch_size = bs_val, shuffle = True)\r\n",
        "val_loader = DataLoader(ld_val, batch_size = bs_val, shuffle = True)"
      ],
      "id": "uiftrqf62usb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x_-1Qe34U5w"
      },
      "source": [
        "# 7. Model\r\n",
        "\r\n"
      ],
      "id": "3x_-1Qe34U5w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yJ7JTk04XIu"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        # Conv2D: 1 input channel, 8 output channels, 3 by 3 kernel, stride of 1.\r\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\r\n",
        "        # change the linear layer to output 3 dim\r\n",
        "        self.fc1 = nn.Linear(87616, 3)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = torch.flatten(x, 1)\r\n",
        "        x = self.fc1(x)\r\n",
        "        output = F.log_softmax(x, dim = 1)\r\n",
        "        return output"
      ],
      "id": "-yJ7JTk04XIu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJKx7kXTQYQN"
      },
      "source": [
        "# Activate gpu\r\n",
        "if torch.cuda.is_available():  \r\n",
        "    print('using GPU')\r\n",
        "    device = \"cuda:0\" \r\n",
        "else:  \r\n",
        "    device = \"cpu\"\r\n",
        "model = Net().to(torch.device(device))"
      ],
      "id": "EJKx7kXTQYQN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbl4-OO417"
      },
      "source": [
        "summary(model, (1, 150, 150))"
      ],
      "id": "zoBbl4-OO417",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E4MDvTLdnP"
      },
      "source": [
        "# 8. Training the model\r\n",
        "\r\n",
        "Reference material: [Towards data science: PyTorch [Tabular] — Multiclass Classification](https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab)"
      ],
      "id": "L1E4MDvTLdnP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98hO2OLpLdRK"
      },
      "source": [
        "def multi_acc(y_pred, y_test):\r\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\r\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\r\n",
        "    correct_pred = (y_pred_tags == y_test).float()\r\n",
        "    acc = correct_pred.sum() / len(correct_pred)\r\n",
        "    \r\n",
        "    return acc"
      ],
      "id": "98hO2OLpLdRK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7yVj4RlPNNV"
      },
      "source": [
        "# Define criterion and optimizer, epoch\r\n",
        "epochs = 1\r\n",
        "lr = 0.0001\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)"
      ],
      "id": "E7yVj4RlPNNV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJxZ00RSPTBy"
      },
      "source": [
        "running_loss = 0\r\n",
        "print_every = 40\r\n",
        "steps = 0 \r\n",
        "start = time.time()\r\n",
        "\r\n",
        "accuracy_stats = {\r\n",
        "    'train': [],\r\n",
        "    \"val\": []\r\n",
        "}\r\n",
        "loss_stats = {\r\n",
        "    'train': [],\r\n",
        "    \"val\": []\r\n",
        "}\r\n",
        "\r\n",
        "for e in range(epochs):\r\n",
        "    train_epoch_loss = 0\r\n",
        "    train_epoch_acc = 0\r\n",
        "    \r\n",
        "    steps = 0\r\n",
        "    model.train()\r\n",
        "    for X_train_batch, y_train_batch in train_loader:\r\n",
        "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\r\n",
        "        \r\n",
        "        steps += 1\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        output = model.forward(X_train_batch)\r\n",
        "        train_loss  = criterion(output, torch.max(y_train_batch, 1)[1])\r\n",
        "        train_acc = multi_acc(output, torch.max(y_train_batch, 1)[1])\r\n",
        "        train_loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        train_epoch_loss += train_loss.item()\r\n",
        "        train_epoch_acc += train_acc.item()\r\n",
        "        \r\n",
        "    with torch.no_grad():\r\n",
        "        val_epoch_loss = 0\r\n",
        "        val_epoch_acc = 0\r\n",
        "        model.eval()\r\n",
        "        for X_val_batch, y_val_batch in val_loader:\r\n",
        "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\r\n",
        "\r\n",
        "            y_val_pred = model.forward(X_val_batch)\r\n",
        "\r\n",
        "            val_loss = criterion(y_val_pred, torch.max(y_val_batch, 1)[1])\r\n",
        "            val_acc = multi_acc(y_val_pred, torch.max(y_val_batch, 1)[1])\r\n",
        "\r\n",
        "            val_epoch_loss += val_loss.item()\r\n",
        "            val_epoch_acc += val_acc.item()\r\n",
        "        \r\n",
        "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\r\n",
        "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\r\n",
        "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\r\n",
        "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\r\n",
        "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\r\n",
        "    print(\"Epoch: {}/{} - {} - \".format(e+1, epochs, now),\r\n",
        "      \"Training Loss: {:.4f} - \".format(train_epoch_loss/len(train_loader)),\r\n",
        "      \"Validation Loss: {:.4f} - \".format(val_epoch_loss/len(val_loader)),\r\n",
        "      \"Validation Accuracy: {:.4f}\".format(val_epoch_acc/len(val_loader))),\r\n",
        "    \"Training Accuracy: {:.4f}\".format(train_epoch_acc/len(train_loader))\r\n",
        "\r\n",
        "# autosave model\r\n",
        "end_model_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\r\n",
        "model_name = 'boilerplate_net'\r\n",
        "checkpoint = {\r\n",
        "    'c_lr': lr,\r\n",
        "    'model_name': model_name,\r\n",
        "    'c_epochs': epochs,\r\n",
        "}\r\n",
        "path = './model_{}_{}_{}'.format(epochs, model_name, end_model_time)\r\n",
        "torch.save(checkpoint, path)"
      ],
      "id": "lJxZ00RSPTBy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k01aNmdgxVX-"
      },
      "source": [
        "# References\r\n",
        "\r\n",
        "Leszczynski, M. (2010). Image Preprocessing for Illumination Invariant Face \r\n",
        "Verification. Journal of telecommunications and information technology, 19-25."
      ],
      "id": "k01aNmdgxVX-"
    }
  ]
}