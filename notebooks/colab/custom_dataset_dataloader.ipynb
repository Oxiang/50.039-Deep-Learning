{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moral-rebound"
   },
   "source": [
    "# Boilerplate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crude-waste"
   },
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pillow\n",
    "from PIL import Image\n",
    "# Torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import precision_score\n",
    "# Misc\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4gBSNDrcfGl"
   },
   "source": [
    "# 1. Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcHbRtuHcd-N"
   },
   "outputs": [],
   "source": [
    "!git clone -b data https://github.com/Oxiang/50.039-Deep-Learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3ElMDwpU831"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4-h2B3OVCRe"
   },
   "outputs": [],
   "source": [
    "cd 50.039-Deep-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DW2ZD2afqdmF"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "(\n",
    "tree dataset -d\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWQTRVbAVkPQ"
   },
   "source": [
    "# 2. Dataset high-level info\n",
    "\n",
    "The images stored in the **./dataset** folder and its subfolder consists of 150 by 150 pixels greyscale images, representing X-Ray pictures of lungs.\n",
    "\n",
    "The images, consists of X-ray pictures of the following:\n",
    "\n",
    "| Description                              | Class index | Tensor  | Class label        |\n",
    "| ---------------------------------------- | ----------- | ------- | ------------------ |\n",
    "| People with no infection diagnosis       | 0           | [1 0 0] | normal             |\n",
    "| People with infected lungs and non-covid | 1           | [0 1 0] | infected_non_covid |\n",
    "| People with infected lungs and covid     | 2           | [0 0 1] | infected_covid     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZhr10rHVlIj"
   },
   "outputs": [],
   "source": [
    "classes = {0: 'normal', 1: 'infected_non_covid', 2: 'infected_covid'}\n",
    "groups = ['train', 'test', 'val']\n",
    "dataset_numbers = {\n",
    "    'train_normal': 1341,\n",
    "    'train_infected_non_covid': 2530,\n",
    "    'train_infected_covid': 1345,\n",
    "    'val_normal': 8,\n",
    "    'val_infected_non_covid': 8,\n",
    "    'val_infected_covid': 8,    \n",
    "    'test_normal': 234,\n",
    "    'test_infected_non_covid': 242,\n",
    "    'test_infected_covid': 138,\n",
    "}\n",
    "dataset_paths = {\n",
    "    'train_normal': './dataset/train/normal/',\n",
    "    'train_infected_non_covid': './dataset/train/infected/non-covid/',\n",
    "    'train_infected_covid': './dataset/train/infected/covid/',\n",
    "    'val_normal': './dataset/val/normal/',\n",
    "    'val_infected_non_covid': './dataset/val/infected/non-covid/',\n",
    "    'val_infected_covid': './dataset/val/infected/covid/',    \n",
    "    'test_normal': './dataset/test/normal/',\n",
    "    'test_infected_non_covid': './dataset/test/infected/non-covid/',\n",
    "    'test_infected_covid': './dataset/test/infected/covid/',    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kakruEOWFn2"
   },
   "source": [
    "View one of the images and its properties. These images consist of a Numpy array, with values ranging between 0 and 255. These values will be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THPZFpETWAWK"
   },
   "outputs": [],
   "source": [
    "path_to_file = './dataset/train/normal/1.jpg'\n",
    "with open(path_to_file, 'rb') as f:\n",
    "    im = np.asarray(Image.open(f))\n",
    "    plt.imshow(im)\n",
    "f.close()\n",
    "print('Image shape is: {}'.format(im.shape))\n",
    "# Images are defined as a Numpy array of values between 0 and 256\n",
    "print('Image as a numpy array is:\\n {}'.format(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcs9foiNXKY1"
   },
   "source": [
    "# 3. Creating a Dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKLCcIvxXLHu"
   },
   "source": [
    "## 3.1 General Dataset object that is custom made for train, val, test to individually use\n",
    "\n",
    "length method ( __ len __ )\n",
    "\n",
    "> return the number of images present in the dataset\n",
    "\n",
    "getitem method ( __ getitem __ )\n",
    "\n",
    "> fetch an image and its label, using a single index value. Returns the image, along with a one-hot vector corresponding to the class of the object. Both returned parameters will be torch tensors.\n",
    "- [1, 0,0] for normal class\n",
    "- [0, 1, 0] for infected_non_covid class\n",
    "- [0, 0, 1] for infected_covid class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtNXQwKbXNfY"
   },
   "outputs": [],
   "source": [
    "class Lung_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Generic Dataset class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, groups, dataset_numbers, dataset_paths):\n",
    "        \"\"\"\n",
    "        Constructor for generic Dataset class - assembles\n",
    "        the important parameters in attributes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        groups : str\n",
    "            Allowed values: train, val, test\n",
    "        dataset_numbers : dict\n",
    "            Count of each class within specified group\n",
    "        dataset_paths : dict\n",
    "            Path to each class within specified group\n",
    "        \"\"\"\n",
    "\n",
    "        self.img_size = (150, 150)\n",
    "        self.classes = {\n",
    "            0: 'normal',\n",
    "            1: 'infected_non_covid',\n",
    "            2: 'infected_covid'\n",
    "        }        \n",
    "        self.groups = groups\n",
    "        self.dataset_numbers = dataset_numbers\n",
    "        self.dataset_paths = dataset_paths\n",
    "        \n",
    "        \n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        Descriptor function.\n",
    "        Will print details about the dataset when called.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate description\n",
    "        msg = \"This is the {} dataset of the Lung Dataset\".format(self.groups)\n",
    "        msg += \" used for the Small Project Demo in the 50.039 Deep Learning class\"\n",
    "        msg += \" in March 2021. \\n\"\n",
    "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\n",
    "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
    "        msg += \"The images are stored in the following locations \"\n",
    "        msg += \"and each one contains the following number of images:\\n\"\n",
    "        for key, val in self.dataset_paths.items():\n",
    "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, val, self.dataset_numbers[key])\n",
    "        print(msg)\n",
    "        \n",
    "    \n",
    "    def open_img(self, group_val, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - group_val should take values in 'train', 'test' or 'val'.\n",
    "        - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \n",
    "        Returns loaded image as a normalized Numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Asserts checking for consistency in passed parameters\n",
    "        err_msg = \"Error - group_val variable should be set to 'train', 'test' or 'val'.\"\n",
    "        assert group_val in self.groups, err_msg\n",
    "        \n",
    "        err_msg = \"Error - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid.\"\n",
    "        assert class_val in self.classes.values(), err_msg\n",
    "        \n",
    "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\n",
    "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
    "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val, max_val)\n",
    "        assert isinstance(index_val, int), err_msg\n",
    "        assert index_val >= 0 and index_val <= max_val, err_msg\n",
    "        \n",
    "        # Open file as before\n",
    "        path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            # Convert to Numpy array and normalize pixel values by dividing by 255.\n",
    "            im = np.asarray(Image.open(f))/255\n",
    "        f.close()\n",
    "        return im\n",
    "    \n",
    "    \n",
    "    def show_img(self, group_val, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens, then displays image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - group_val should take values in 'train', 'test' or 'val'.\n",
    "        - class_val variable should be set to 'normal' or 'infected'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Open image\n",
    "        im = self.open_img(group_val, class_val, index_val)\n",
    "        \n",
    "        # Display\n",
    "        plt.imshow(im)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length special method, returns the number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Length function\n",
    "        return sum(self.dataset_numbers.values())\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Getitem special method.\n",
    "        \n",
    "        Expects an integer value index, between 0 and len(self) - 1.\n",
    "        \n",
    "        Returns the image and its label as a one hot vector, both\n",
    "        in torch tensor format in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get item special method\n",
    "        first_val = int(list(self.dataset_numbers.values())[0])\n",
    "        second_val = int(list(self.dataset_numbers.values())[1])\n",
    "        if index < first_val:\n",
    "            class_val = 'normal'\n",
    "            label = torch.Tensor([1, 0, 0])\n",
    "        elif index < (first_val+second_val):\n",
    "            class_val = 'infected_non_covid'\n",
    "            index = index - first_val\n",
    "            label = torch.Tensor([0, 1, 0])\n",
    "        else:\n",
    "            class_val = \"infected_covid\"\n",
    "            index = index - (first_val+second_val)\n",
    "            label = torch.Tensor([0, 0, 1])\n",
    "        im = self.open_img(self.groups, class_val, index)\n",
    "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcOYB7aYX9O-"
   },
   "outputs": [],
   "source": [
    "dataset_numbers = {\n",
    "    'train': {\n",
    "        'train_normal': 1341,\n",
    "        'train_infected_non_covid': 2530,\n",
    "        'train_infected_covid': 1345,\n",
    "    },\n",
    "    'val': {\n",
    "        'val_normal': 8,\n",
    "        'val_infected_non_covid': 8,\n",
    "        'val_infected_covid': 8,\n",
    "    },\n",
    "    'test': {\n",
    "        'test_normal': 234,\n",
    "        'test_infected_non_covid': 242,\n",
    "        'test_infected_covid': 138,\n",
    "    }\n",
    "}\n",
    "dataset_paths = {\n",
    "    'train': {\n",
    "        'train_normal': './dataset/train/normal/',\n",
    "        'train_infected_non_covid': './dataset/train/infected/non-covid/',\n",
    "        'train_infected_covid': './dataset/train/infected/covid/',\n",
    "    },\n",
    "    'val': {\n",
    "        'val_normal': './dataset/val/normal/',\n",
    "        'val_infected_non_covid': './dataset/val/infected/non-covid/',\n",
    "        'val_infected_covid': './dataset/val/infected/covid/',\n",
    "    },\n",
    "    'test': {\n",
    "        'test_normal': './dataset/test/normal/',\n",
    "        'test_infected_non_covid': './dataset/test/infected/non-covid/',\n",
    "        'test_infected_covid': './dataset/test/infected/covid/',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS4ivSxsYKgl"
   },
   "outputs": [],
   "source": [
    "def verify_dataset(group,dataset,image_overall_index=7,class_val='normal',\n",
    "                   image_specific_dataset_index=1):\n",
    "  print('Verify the special methods __len__ and __get_item__')\n",
    "  print('Number of images in {} dataset: {}'.format(group, len(dataset)))\n",
    "  print('Details for image id {} from the {} dataset'.format(\n",
    "      image_overall_index,\n",
    "      group\n",
    "  ))\n",
    "  im, class_oh = dataset[image_overall_index]\n",
    "  print('Sample image shape: {}'.format(im.shape))\n",
    "  print('Sample image: {}'.format(im))\n",
    "  print('Sample image class: {}'.format(class_oh))\n",
    "\n",
    "  print('\\nVerify the open_img and show_img functions')\n",
    "  print('Open and show image {} from the {}_{} dataset'.format(\n",
    "      image_specific_dataset_index,\n",
    "      group,\n",
    "      class_val\n",
    "  ))\n",
    "  im = dataset.open_img(group, class_val, image_specific_dataset_index)\n",
    "  print('Same sample image shape: {}'.format(im.shape))\n",
    "  print('Same sample image: {}'.format(im))\n",
    "  dataset.show_img(group, class_val, image_specific_dataset_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9CO29iBYHs-"
   },
   "source": [
    "## 3.2 Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBI4uwQqYiWI"
   },
   "outputs": [],
   "source": [
    "train_group = 'train'\n",
    "ld_train = Lung_Dataset(\n",
    "    train_group,\n",
    "    dataset_numbers[train_group],\n",
    "    dataset_paths[train_group]\n",
    ")\n",
    "ld_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jo4ZJTwpYjw4"
   },
   "outputs": [],
   "source": [
    "verify_dataset(train_group,ld_train,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMz0d7zfY5W7"
   },
   "source": [
    "## 3.3 Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4Sju4KFY5p8"
   },
   "outputs": [],
   "source": [
    "val_group = 'val'\n",
    "ld_val = Lung_Dataset(\n",
    "    val_group,\n",
    "    dataset_numbers[val_group],\n",
    "    dataset_paths[val_group]\n",
    ")\n",
    "ld_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzhsBRE4Y_F_"
   },
   "outputs": [],
   "source": [
    "verify_dataset(val_group,ld_val,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojan7nynZILA"
   },
   "source": [
    "## 3.4 Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdxD1NzmZIvq"
   },
   "outputs": [],
   "source": [
    "test_group = 'test'\n",
    "ld_test = Lung_Dataset(\n",
    "    test_group,\n",
    "    dataset_numbers[test_group],\n",
    "    dataset_paths[test_group]\n",
    ")\n",
    "ld_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49ohnFncZJJD"
   },
   "outputs": [],
   "source": [
    "verify_dataset(test_group,ld_test,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XwbFL6DbSxg"
   },
   "source": [
    "# 4. Data visualization\n",
    "\n",
    "This requires a `grouped bar chart`. Refer to [matplotlib Grouped bar chart with labels](https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html) for starter code\n",
    "\n",
    "<u>**Discuss whether or not the dataset is balanced between classes, uniformly distributed, etc.**</u>\n",
    "\n",
    "**Training set**\n",
    "\n",
    "The train data for the different classes are imbalanced. From the graph plotted below, the `infected_non_covid` class has significantly more data points than the other classes. Overall, the ratio `normal:infected_non_covid:infected_covid` is approximately `1:2:1`.\n",
    "\n",
    "This could present more complications if the model is trained in a stacking manner by first training normal vs infected. The ratio of `normal:infected` would be a ratio of `1:3` which is more imbalanced.\n",
    "\n",
    "**Testing set**\n",
    "\n",
    "The test set is also slightly imbalanced with the ratio `normal:infected_non_covid:infected_covid` being approximately `2:2:1`. However, this is not as bad as an imbalanced training set because the test set will not affect the model's parameter tuning.\n",
    "\n",
    "**Validation set**\n",
    "\n",
    "The val set is uniformly distributed between the three classes. However, it is glaring that there are only 8 validation samples for the 3 classes. Considering the ratio of `train:val:test`, the number of validation samples is far too low. For example. with reference to the infected_non_covid class, the ratio of `train:val:test` is `316:1:30`. which is quite far off from the recommended ratios like `80:10:10` or `8:1:1` as indicated by [Stanford's CS230](https://cs230.stanford.edu/blog/split/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-DIZ_jPbVFh"
   },
   "outputs": [],
   "source": [
    "labels = ['normal', 'infected_non_covid', 'infected_covid']\n",
    "\n",
    "train_normal_inc_ic = list(ld_train.dataset_numbers.values())\n",
    "val_normal_inc_ic = list(ld_val.dataset_numbers.values())\n",
    "test_normal_inc_ic = list(ld_test.dataset_numbers.values())\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "rects1 = ax.bar(x - width, train_normal_inc_ic, width, label='train')\n",
    "rects2 = ax.bar(x + width, val_normal_inc_ic, width, label='val')\n",
    "rects3 = ax.bar(x, test_normal_inc_ic, width, label='test')\n",
    "\n",
    "ax.set_ylabel('Number of datapoints')\n",
    "ax.set_title('Number of datapoints with respect to each dataset and class')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFl4rsh5p1rX"
   },
   "source": [
    "# 4. Why normalize the data?\n",
    "\n",
    "To recap the normalization can be found in the `Lung_Dataset` class' `open_img` function which did the following normalization.\n",
    "\n",
    "```python\n",
    "# Convert to Numpy array and normalize pixel values by dividing by 255.\n",
    "im = np.asarray(Image.open(f))/255\n",
    "```\n",
    "\n",
    "Images have RGB ranges from 0-255. Considering various activation functions like `sigmoid` such a large range would mean that for vastly different values like 100 and 255, not much difference can be seen when passed into the `sigmoid` activation function. Both would produce a value that is close to 1.\n",
    "\n",
    "Taking the same values as reference, if we divide by 255, for a value of 100,  $\\frac{100}{255}$ we get approximately 0.39. Then for a value of 255, $\\frac{255}{255}$ we get 1. For the initial value of 100 that becomes 0.39 after the division, passing it into `sigmoid(0.39)` produces a value of 0.596. Meanwhile for the initial value of 255 that becomes 1 after division, passing it into `sigmoid(1)` produces a value of 0.731. This difference in value allows us to extract meaningful differences in the pixel values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6jfWGpZqCIU"
   },
   "source": [
    "# 5. Other possible pre-processing\n",
    "\n",
    "Form the plot below, which is based on the Training set for normal images as reference, it is evident that there are several differences in the photo dimensions and photo environment. \n",
    "\n",
    "For example, comparing image_index 1 and image_index 28 there is a clear difference in the lighting, Image_index 28 is a lot brighter. One pre-processing step could be to use histogram normalization. There is a paper that recommends 14 possible normalization algorithms that can be performed (Leszczynski, 2010)\n",
    "\n",
    "Aother example is comparing \"skinny\" images like image_index 1 and image_index 31 where there is significantly more dark backgrounds at the side compares to images like image_index 12. Perhaps a edge detection algorithm can be applied to just filter the relevant parts of the image which are the lungs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgyLkZ9xp2GH"
   },
   "outputs": [],
   "source": [
    "row = 2\n",
    "col = 2\n",
    "\n",
    "selected_indices = [1,12,28,31]\n",
    "f, axarr = plt.subplots(row,col,figsize=(10,7))\n",
    "counter = 0\n",
    "for row_index in range(row):\n",
    "  for col_index in range(col):\n",
    "    image_index = selected_indices[counter]\n",
    "    im = ld_train.open_img('train', 'normal', image_index)\n",
    "    axarr[row_index,col_index].set_title('Image index: {}'.format(image_index))\n",
    "    axarr[row_index,col_index].imshow(im)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC14yqAz2vSI"
   },
   "source": [
    "# 6. Creating a data loader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiftrqf62usb"
   },
   "outputs": [],
   "source": [
    "bs_train = 64\n",
    "bs_test = 24\n",
    "bs_val = 1\n",
    "train_loader = DataLoader(ld_train, batch_size = bs_train, shuffle = True)\n",
    "test_loader = DataLoader(ld_test, batch_size = bs_test, shuffle = True)\n",
    "val_loader = DataLoader(ld_val, batch_size = bs_val, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x_-1Qe34U5w"
   },
   "source": [
    "# 7. Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yJ7JTk04XIu"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Conv2D: 1 input channel, 8 output channels, 3 by 3 kernel, stride of 1.\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, 1)\n",
    "        # change the linear layer to output 3 dim\n",
    "        self.fc1 = nn.Linear(175232, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim = 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TBNN, self).__init__()\n",
    "        # in channels, out channels, kernel size, stride \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=48, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=48, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=64, out_channels=80, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.pool2 = nn.MaxPool2d(3,3)\n",
    "        self.fc1 = nn.Linear(80*9*9, 64)\n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.gap = nn.AvgPool2d(2)\n",
    "        \n",
    "        # initialization\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv4.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv5.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv6.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv7.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv8.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x) \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x) \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv4(x) \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv5(x) \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv6(x) \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv7(x) \n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv8(x) \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.gap(x)\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJKx7kXTQYQN"
   },
   "outputs": [],
   "source": [
    "# Activate gpu\n",
    "if torch.cuda.is_available():  \n",
    "    print('using GPU')\n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"\n",
    "model = Net().to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoBbl4-OO417"
   },
   "outputs": [],
   "source": [
    "summary(model, (1, 150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1E4MDvTLdnP"
   },
   "source": [
    "# 8. Training the model\n",
    "\n",
    "Reference material: [Towards data science: PyTorch [Tabular] â€” Multiclass Classification](https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98hO2OLpLdRK"
   },
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7yVj4RlPNNV"
   },
   "outputs": [],
   "source": [
    "# Define criterion and optimizer, epoch\n",
    "epochs = 15\n",
    "lr = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJxZ00RSPTBy"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "start_model_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "model_name = 'boilerplate_net'\n",
    "path = './checkpoint_model_{}_{}_{}'.format(epochs, model_name, start_model_time)\n",
    "\n",
    "accuracy_stats_epoch = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "loss_stats_epoch = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model.forward(X_train_batch)\n",
    "        train_loss  = criterion(output, torch.max(y_train_batch, 1)[1])\n",
    "        train_acc = multi_acc(output, torch.max(y_train_batch, 1)[1])\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_acc = 0\n",
    "        model.eval()\n",
    "        for X_test_batch, y_test_batch in test_loader:\n",
    "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "\n",
    "            y_test_pred = model.forward(X_test_batch)\n",
    "\n",
    "            test_loss = criterion(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
    "            test_acc = multi_acc(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
    "\n",
    "            test_epoch_loss += test_loss.item()\n",
    "            test_epoch_acc += test_acc.item()\n",
    "\n",
    "    # Epoch metrics\n",
    "    loss_stats_epoch['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats_epoch['test'].append(test_epoch_loss/len(test_loader))\n",
    "    loss_stats_epoch['epoch'].append(e+1)\n",
    "    accuracy_stats_epoch['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats_epoch['test'].append(test_epoch_acc/len(test_loader))\n",
    "    accuracy_stats_epoch['epoch'].append(e+1)\n",
    "\n",
    "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"Epoch: {}/{} - {} - \".format(e+1, epochs, now),\n",
    "          \"Training Loss: {:.4f} - \".format(train_epoch_loss/len(train_loader)),\n",
    "          \"Training Accuracy: {:.4f} -\".format(train_epoch_acc/len(train_loader)),\n",
    "          \"Test Loss: {:.4f} - \".format(test_epoch_loss/len(val_loader)),\n",
    "          \"Test Accuracy: {:.4f}\".format(test_epoch_acc/len(val_loader)))\n",
    "    \n",
    "    # auto-save a checkpoint at every epoch for evaluation and continual training. \n",
    "    # Overrides the previous checkpoint for the same training\n",
    "    checkpoint = {\n",
    "        'epochs': e+1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_epoch_loss/len(train_loader),\n",
    "        'test_loss': test_epoch_loss/len(test_loader),\n",
    "        'test_accuracy': test_epoch_acc/len(test_loader),\n",
    "        'training_accuracy': train_epoch_acc/len(train_loader)\n",
    "    }\n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUG2wrXboT8T"
   },
   "source": [
    "# 9. Graph visualizations of learning curves\n",
    "\n",
    "Graphs covered\n",
    "\n",
    "1. Accuracy against steps\n",
    "2. Recall against steps\n",
    "3. Precision against steps\n",
    "4. F1 against steps\n",
    "5. Loss against steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhUVlMGJoW0N"
   },
   "outputs": [],
   "source": [
    "def plot_graph(data,datasets,y_axis,x_axis):\n",
    "  \"\"\"Plots a graph of y against x\n",
    "\n",
    "    Keyword arguments:\n",
    "    data -- dictionary with keys: train, val, epoch, steps\n",
    "    datasets -- list of datasets to plot: train, val\n",
    "    y_axis -- name of the y_axis. example: accuracy\n",
    "    x_axis -- either epoch or steps\n",
    "    \"\"\"\n",
    "  plt.figure(figsize=(17,8))\n",
    "  for dataset in datasets:\n",
    "    target_y = data[dataset]\n",
    "    target_x = data[x_axis]\n",
    "    plt.plot(target_x, target_y, label = dataset)\n",
    "  plt.xlabel(x_axis)\n",
    "  plt.ylabel(y_axis)\n",
    "  plt.title('Graph of {} against {}'.format(y_axis, x_axis))\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWJD3RFbcyxd"
   },
   "outputs": [],
   "source": [
    "plot_graph(accuracy_stats_epoch, ['train', 'test'], 'accuracy', 'epoch')\n",
    "plot_graph(loss_stats_epoch, ['train', 'test'], 'loss', 'epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Load Model for testing Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for evaluation\n",
    "\n",
    "# input checkpoint's path, if not use the path that was created while training\n",
    "# checkpoint_path = ''\n",
    "# if checkpoint_path == \"\":\n",
    "#     checkpoint_path = path\n",
    "    \n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# model = Net().to(torch.device(device))\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.eval()\n",
    "\n",
    "acc = 0\n",
    "for X_val_batch, y_val_batch in val_loader:\n",
    "    X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "    y_val_pred = model.forward(X_val_batch)\n",
    "    \n",
    "    val_acc = multi_acc(y_val_pred, torch.max(y_val_batch, 1)[1])\n",
    "    acc += val_acc.item()\n",
    "\n",
    "print(\"Validation accuracy\", acc/len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGpqG8Y6es_7"
   },
   "source": [
    "# 11. Validation set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ku1pIryfPga"
   },
   "outputs": [],
   "source": [
    "def get_ground_truth_prediction(data_loader):\n",
    "    '''Gets the predicted & ground truth labels for each image in data_loader\n",
    "\n",
    "    Keyword arguments:\n",
    "    data_loader -- data loader object\n",
    "    '''\n",
    "    correct = 0\n",
    "    result = {\n",
    "        'im': [],\n",
    "        'ground_truth': [],\n",
    "        'prediction': [],\n",
    "        'index': []\n",
    "    }\n",
    "    for batch_idx, (X_val_batch, ground_truth) in enumerate(data_loader):\n",
    "        X_val_batch, ground_truth = X_val_batch.to(device), ground_truth.to(device)\n",
    "        ground_truth = torch.max(ground_truth,1)[1].item()\n",
    "        ground_truth = ld_val.classes.get(ground_truth)\n",
    "        prediction = model.forward(X_val_batch)\n",
    "        prediction = torch.max(prediction,1)[1].item()\n",
    "        prediction = ld_val.classes.get(prediction)\n",
    "        if ground_truth == prediction:\n",
    "          correct += 1\n",
    "        im, class_oh = ld_val[batch_idx]\n",
    "        result['im'].append(im)\n",
    "        result['ground_truth'].append(ground_truth)\n",
    "        result['prediction'].append(prediction)\n",
    "        result['index'].append(batch_idx)\n",
    "    return result, correct\n",
    "\n",
    "def plot_ground_truth_prediction(row, col, result):\n",
    "    '''Plots the subplots of images with their ground truth and prediction\n",
    "\n",
    "    Keyword arguments:\n",
    "    row -- integer value for number of rows of plot\n",
    "    col -- integer value for number of columns of plot\n",
    "    result -- dictionary containing index, prediction, ground truth, image\n",
    "    '''\n",
    "    f, axarr = plt.subplots(row,col,figsize=(20,30))\n",
    "    f.suptitle('Validation set pictures, with predicted and ground truth labels.\\nAverage performance {}/{}={}%'.format(correct,total,accuracy))\n",
    "    counter = 0\n",
    "    for row_index in range(row):\n",
    "        for col_index in range(col):\n",
    "            axarr[row_index,col_index].set_title(\n",
    "                'Ground truth label: {}\\nPredicted label: {}'.format(\n",
    "                    result['ground_truth'][counter],\n",
    "                    result['prediction'][counter])\n",
    "                )\n",
    "            axarr[row_index,col_index].imshow(torch.squeeze(result['im'][counter]))\n",
    "            counter += 1\n",
    "\n",
    "val_loader = DataLoader(ld_val, batch_size = 1, shuffle = False)\n",
    "total = len(val_loader)\n",
    "result, correct = get_ground_truth_prediction(val_loader)\n",
    "accuracy = round(correct/total,3) * 100\n",
    "row = 6\n",
    "col = 4\n",
    "\n",
    "plot_ground_truth_prediction(row, col, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Bagging with two binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L0 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L0_Lung_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, groups, dataset_numbers, dataset_paths, breakdown):\n",
    "        \"\"\"\n",
    "        Constructor for generic Dataset class - simply assembles\n",
    "        the important parameters in attributes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # All images are of size 150 x 150\n",
    "        self.img_size = (150, 150)\n",
    "        \n",
    "        # Only two classes will be considered here (normal and infected)\n",
    "        self.classes = {0: 'normal', 1: 'infected'}\n",
    "        \n",
    "        # covid variables\n",
    "        self.covid = {0: '', 1: 'covid', 2:'non-covid'}\n",
    "        \n",
    "        # differentiate the infected\n",
    "        self.breakdown = breakdown\n",
    "        \n",
    "        # The dataset consists only of training images\n",
    "        self.groups = groups\n",
    "        \n",
    "        # Number of images in each part of the dataset\n",
    "        self.dataset_numbers = dataset_numbers\n",
    "        \n",
    "        # Path to images for different parts of the dataset\n",
    "        self.dataset_paths = dataset_paths\n",
    "        \n",
    "        \n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        Descriptor function.\n",
    "        Will print details about the dataset when called.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate description\n",
    "        msg = \"This is the training dataset of the Lung Dataset\"\n",
    "        msg += \" used for the Small Project Demo in the 50.039 Deep Learning class\"\n",
    "        msg += \" in Feb-March 2021. \\n\"\n",
    "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\n",
    "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
    "        msg += \"The images are stored in the following locations \"\n",
    "        msg += \"and each one contains the following number of images:\\n\"\n",
    "        for key, val in self.dataset_paths.items():\n",
    "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, val, self.dataset_numbers[key])\n",
    "        print(msg)\n",
    "        \n",
    "    \n",
    "    def open_img(self, group_val, class_val, covid, index_val):\n",
    "        \"\"\"\n",
    "        Opens image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - group_val should take values in 'train', 'test' or 'val'.\n",
    "        - class_val variable should be set to 'normal' or 'infected'.\n",
    "        - covid variable should be '', 'covid' or 'non-covid'\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \n",
    "        Returns loaded image as a normalized Numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Asserts checking for consistency in passed parameters\n",
    "        err_msg = \"Error - group_val variable should be set to 'train', 'test' or 'val'.\"\n",
    "        assert group_val in self.groups, err_msg\n",
    "        \n",
    "        err_msg = \"Error - class_val variable should be set to 'normal' or 'infected'.\"\n",
    "        assert class_val in self.classes.values(), err_msg\n",
    "        \n",
    "        err_msg = \"Error - class_val variable should be set to '', 'covid' or 'non-covid'.\"\n",
    "        assert covid in self.covid.values(), err_msg      \n",
    "        \n",
    "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\n",
    "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
    "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val, max_val)\n",
    "        assert isinstance(index_val, int), err_msg\n",
    "        assert index_val >= 0 and index_val <= max_val, err_msg\n",
    "        \n",
    "        # Open file as before\n",
    "        if covid == \"\":\n",
    "            path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\n",
    "        else:\n",
    "            path_to_file = '{}/{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], covid, index_val)\n",
    "\n",
    "#         print('path = ', path_to_file)\n",
    "            \n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            im = np.asarray(Image.open(f))/255\n",
    "        f.close()\n",
    "        return im\n",
    "    \n",
    "    \n",
    "    def show_img(self, group_val, class_val, covid, index_val):\n",
    "        \"\"\"\n",
    "        Opens, then displays image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - group_val should take values in 'train', 'test' or 'val'.\n",
    "        - class_val variable should be set to 'normal' or 'infected'.\n",
    "        - covid variable should be '', 'covid' or 'non-covid'\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Open image\n",
    "        im = self.open_img(group_val, class_val, covid, index_val)\n",
    "        \n",
    "        # Display\n",
    "        plt.imshow(im)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length special method, returns the number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Length function\n",
    "        return sum(self.dataset_numbers.values())\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Getitem special method.\n",
    "        \n",
    "        Expects an integer value index, between 0 and len(self) - 1.\n",
    "        \n",
    "        Returns the image and its label as a one hot vector, both\n",
    "        in torch tensor format in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get item special method\n",
    "        first_val = int(list(self.dataset_numbers.values())[0])\n",
    "        if index < first_val:\n",
    "            class_val = 'normal'\n",
    "            label = torch.Tensor([1, 0])\n",
    "            covid = \"\"\n",
    "        else:\n",
    "            class_val = 'infected'\n",
    "            index = index - first_val\n",
    "            label = torch.Tensor([0, 1])\n",
    "            breakdown_first_val = int(list(self.breakdown.values())[0]) # covid\n",
    "            if index < breakdown_first_val:\n",
    "                class_val = 'infected'\n",
    "                covid = 'covid'\n",
    "            else:\n",
    "                class_val = 'infected'\n",
    "                index = index - breakdown_first_val\n",
    "                covid = 'non-covid'\n",
    "        im = self.open_img(self.groups, class_val, covid, index)\n",
    "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dataset_paths = {\n",
    "    'layer_0': {\n",
    "        'train': {\n",
    "            'train_normal':'./dataset/train/normal',\n",
    "            'train_infected': './dataset/train/infected'\n",
    "        },\n",
    "        'val': {\n",
    "            'val_normal':'./dataset/val/normal',\n",
    "            'val_infected': './dataset/val/infected'\n",
    "        },\n",
    "        'test': {\n",
    "            'test_normal':'./dataset/test/normal',\n",
    "            'test_infected': './dataset/test/infected'\n",
    "        }\n",
    "    },\n",
    "    'layer_1':{\n",
    "        'train': {\n",
    "            'train_covid': './dataset/train/infected/covid',\n",
    "            'train_non_covid' : './dataset/train/infected/non-covid'\n",
    "        },\n",
    "        'val': {\n",
    "            'val_covid': './dataset/val/infected/covid',\n",
    "            'val_non_covid' : './dataset/val/infected/non-covid'\n",
    "        },\n",
    "        'test': {\n",
    "            'test_covid': './dataset/test/infected/covid',\n",
    "            'test_non_covid' : './dataset/test/infected/non-covid'            \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "binary_dataset_numbers = {\n",
    "    'layer_0': {\n",
    "        'train': {\n",
    "            'train_normal': 1341,\n",
    "            'train_infected': 3875\n",
    "        },\n",
    "        'val': {\n",
    "            'val_normal': 8,\n",
    "            'val_infected': 16\n",
    "        },\n",
    "        'test': {\n",
    "            'test_normal': 234,\n",
    "            'test_infected': 380\n",
    "        }\n",
    "    },\n",
    "    'layer_1':{\n",
    "        'train': {\n",
    "            'train_covid': 1345,\n",
    "            'train_non_covid' : 2530\n",
    "        },\n",
    "        'val': {\n",
    "            'val_covid': 8,\n",
    "            'val_non_covid' : 8\n",
    "        },\n",
    "        'test': {\n",
    "            'test_covid': 138,\n",
    "            'test_non_covid' : 242            \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l0_verify_dataset(group,dataset,image_overall_index=7,class_val='normal',\n",
    "                   image_specific_dataset_index=1,covid=\"covid\"):\n",
    "  print('Verify the special methods __len__ and __get_item__')\n",
    "  print('Number of images in {} dataset: {}'.format(group, len(dataset)))\n",
    "  print('Details for image id {} from the {} dataset'.format(\n",
    "      image_overall_index,\n",
    "      group\n",
    "  ))\n",
    "  im, class_oh = dataset[image_overall_index]\n",
    "  print('Sample image shape: {}'.format(im.shape))\n",
    "  print('Sample image: {}'.format(im))\n",
    "  print('Sample image class: {}'.format(class_oh))\n",
    "\n",
    "  print('\\nVerify the open_img and show_img functions')\n",
    "  print('Open and show image {} from the {}_{}/{} dataset'.format(\n",
    "      image_specific_dataset_index,\n",
    "      group,\n",
    "      class_val,\n",
    "      covid\n",
    "  ))\n",
    "  im = dataset.open_img(group, class_val, covid, image_specific_dataset_index)\n",
    "  print('Same sample image shape: {}'.format(im.shape))\n",
    "  print('Same sample image: {}'.format(im))\n",
    "  dataset.show_img(group, class_val, covid, image_specific_dataset_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L0 Train\n",
    "train_group = 'train'\n",
    "l0_ld_train = L0_Lung_Dataset(groups = train_group, \n",
    "                              dataset_numbers = binary_dataset_numbers['layer_0'][train_group], \n",
    "                              dataset_paths = binary_dataset_paths['layer_0'][train_group],\n",
    "                              breakdown = binary_dataset_numbers['layer_1'][train_group])\n",
    "l0_ld_train.describe()\n",
    "# l0_verify_dataset(group = train_group,\n",
    "#                   dataset = l0_ld_train,\n",
    "#                   image_overall_index = 2685,\n",
    "#                   class_val = 'infected')\n",
    "train_paths = []\n",
    "\n",
    "# You can validate the paths from here\n",
    "# for i in range(5212):\n",
    "#     print(l0_ld_train[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L0 Val\n",
    "val_group = 'val'\n",
    "l0_ld_val = L0_Lung_Dataset(groups = val_group, \n",
    "                              dataset_numbers = binary_dataset_numbers['layer_0'][val_group], \n",
    "                              dataset_paths = binary_dataset_paths['layer_0'][val_group],\n",
    "                              breakdown = binary_dataset_numbers['layer_1'][val_group])\n",
    "l0_ld_val.describe()\n",
    "# l0_verify_dataset(group = val_group,\n",
    "#                   dataset = l0_ld_val,\n",
    "#                   image_overall_index = 1,\n",
    "#                   class_val = 'infected')\n",
    "\n",
    "# You can validate the paths from here\n",
    "# for i in range(24):\n",
    "#     print(l0_ld_val[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L0 Test\n",
    "test_group = 'test'\n",
    "l0_ld_test = L0_Lung_Dataset(groups = test_group, \n",
    "                              dataset_numbers = binary_dataset_numbers['layer_0'][test_group], \n",
    "                              dataset_paths = binary_dataset_paths['layer_0'][test_group],\n",
    "                              breakdown = binary_dataset_numbers['layer_1'][test_group])\n",
    "l0_ld_test.describe()\n",
    "# l0_verify_dataset(group = test_group,\n",
    "#                   dataset = l0_ld_test,\n",
    "#                   image_overall_index = 1,\n",
    "#                   class_val = 'infected')\n",
    "# You can validate the paths from here\n",
    "# for i in range(614):\n",
    "#     print(l0_ld_test[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L0 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_bs_train = 64\n",
    "l0_bs_test = 64\n",
    "l0_bs_val = 1\n",
    "l0_train_loader = DataLoader(l0_ld_train, batch_size = l0_bs_train, shuffle = True)\n",
    "l0_test_loader = DataLoader(l0_ld_test, batch_size = l0_bs_test, shuffle = True)\n",
    "l0_val_loader = DataLoader(l0_ld_val, batch_size = l0_bs_val, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L0  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L0_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L0_Net, self).__init__()\n",
    "        # Conv2D: 1 input channel, 8 output channels, 3 by 3 kernel, stride of 1.\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, 1)\n",
    "        # change the linear layer to output 3 dim\n",
    "        self.fc1 = nn.Linear(175232, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim = 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate gpu\n",
    "if torch.cuda.is_available():  \n",
    "    print('using GPU')\n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"\n",
    "l0_model = L0_Net().to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(l0_model, (1, 150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criterion and optimizer, epoch\n",
    "epochs = 15\n",
    "lr = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(l0_model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "start_model_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "model_name = 'binary_L0_net'\n",
    "path = './checkpoint_model_{}_{}_{}'.format(epochs, model_name, start_model_time)\n",
    "\n",
    "l0_accuracy_stats_epoch = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "l0_loss_stats_epoch = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    \n",
    "    l0_model.train()\n",
    "    for X_train_batch, y_train_batch in l0_train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = l0_model.forward(X_train_batch)\n",
    "        train_loss  = criterion(output, torch.max(y_train_batch, 1)[1])\n",
    "        train_acc = multi_acc(output, torch.max(y_train_batch, 1)[1])\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "\n",
    "    # testing\n",
    "    with torch.no_grad():\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_acc = 0\n",
    "        l0_model.eval()\n",
    "        for X_test_batch, y_test_batch in l0_test_loader:\n",
    "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "\n",
    "            y_test_pred = l0_model.forward(X_test_batch)\n",
    "\n",
    "            test_loss = criterion(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
    "            test_acc = multi_acc(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
    "\n",
    "            test_epoch_loss += test_loss.item()\n",
    "            test_epoch_acc += test_acc.item()\n",
    "            \n",
    "    \n",
    "    # averaged\n",
    "    train_epoch_loss = train_epoch_loss/len(l0_train_loader)\n",
    "    train_epoch_acc = train_epoch_acc/len(l0_train_loader)\n",
    "    test_epoch_loss = test_epoch_loss/len(l0_test_loader)\n",
    "    test_epoch_acc = test_epoch_acc/len(l0_test_loader)\n",
    "    \n",
    "    # The step number corresponds to the number of batches seen\n",
    "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"Epoch: {}/{} - {} - \".format(e+1, epochs, now),\n",
    "      \"Training Loss: {:.4f} - \".format(train_epoch_loss),\n",
    "      \"Training Accuracy: {:.4f}\".format(train_epoch_acc),\n",
    "      \"Test Loss: {:.4f} - \".format(test_epoch_loss),\n",
    "      \"Test Accuracy: {:.4f}\".format(test_epoch_acc))\n",
    "    l0_model.train()\n",
    "    \n",
    "    # Epoch metrics\n",
    "    l0_loss_stats_epoch['train'].append(train_epoch_loss)\n",
    "    l0_loss_stats_epoch['test'].append(test_epoch_loss)\n",
    "    l0_loss_stats_epoch['epoch'].append(e+1)\n",
    "    l0_accuracy_stats_epoch['train'].append(train_epoch_acc)\n",
    "    l0_accuracy_stats_epoch['test'].append(test_epoch_acc)\n",
    "    l0_accuracy_stats_epoch['epoch'].append(e+1)\n",
    "    \n",
    "    # auto-save a checkpoint at every epoch for evaluation and continual training. Overrides the previous checkpoint\n",
    "    checkpoint = {\n",
    "        'epochs': e+1,\n",
    "        'model_state_dict': l0_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_epoch_loss,\n",
    "        'test_loss': test_epoch_loss,\n",
    "        'test_accuracy': test_epoch_acc,\n",
    "        'training_accuracy': train_epoch_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L0 Training Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(l0_accuracy_stats_epoch, ['train', 'test'], 'accuracy', 'epoch')\n",
    "plot_graph(l0_loss_stats_epoch, ['train', 'test'], 'loss', 'epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "l0_model.eval()\n",
    "for X_val_batch, y_val_batch in l0_val_loader:\n",
    "    X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "    y_val_pred = l0_model.forward(X_val_batch)\n",
    "    \n",
    "    val_acc = multi_acc(y_val_pred, torch.max(y_val_batch, 1)[1])\n",
    "    acc += val_acc.item()\n",
    "\n",
    "print(\"Validation accuracy for layer 0 net\", acc/len(l0_val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1_Lung_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, groups, dataset_numbers, dataset_paths):\n",
    "        \"\"\"\n",
    "        Constructor for generic Dataset class - simply assembles\n",
    "        the important parameters in attributes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # All images are of size 150 x 150\n",
    "        self.img_size = (150, 150)\n",
    "        \n",
    "        # Only two classes will be considered here (normal and infected)\n",
    "        self.classes = {0: 'covid', 1: 'non_covid'}\n",
    "        \n",
    "        # The dataset consists only of training images\n",
    "        self.groups = groups\n",
    "        \n",
    "        # Number of images in each part of the dataset\n",
    "        self.dataset_numbers = dataset_numbers\n",
    "        \n",
    "        # Path to images for different parts of the dataset\n",
    "        self.dataset_paths = dataset_paths\n",
    "        \n",
    "        \n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        Descriptor function.\n",
    "        Will print details about the dataset when called.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate description\n",
    "        msg = \"This is the training dataset of the Lung Dataset\"\n",
    "        msg += \" used for the Small Project Demo in the 50.039 Deep Learning class\"\n",
    "        msg += \" in Feb-March 2021. \\n\"\n",
    "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\n",
    "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
    "        msg += \"The images are stored in the following locations \"\n",
    "        msg += \"and each one contains the following number of images:\\n\"\n",
    "        for key, val in self.dataset_paths.items():\n",
    "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, val, self.dataset_numbers[key])\n",
    "        print(msg)\n",
    "        \n",
    "    \n",
    "    def open_img(self, group_val, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - group_val should take values in 'train', 'test' or 'val'.\n",
    "        - class_val variable should be set to 'covid' or 'non-covid'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \n",
    "        Returns loaded image as a normalized Numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Asserts checking for consistency in passed parameters\n",
    "        err_msg = \"Error - group_val variable should be set to 'train', 'test' or 'val'.\"\n",
    "        assert group_val in self.groups, err_msg\n",
    "        \n",
    "        err_msg = \"Error - class_val variable should be set to 'covid' or 'non-covid'.\"\n",
    "        assert class_val in self.classes.values(), err_msg     \n",
    "        \n",
    "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\n",
    "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
    "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val, max_val)\n",
    "        assert isinstance(index_val, int), err_msg\n",
    "        assert index_val >= 0 and index_val <= max_val, err_msg\n",
    "        \n",
    "        # Open file as before\n",
    "        path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\n",
    "\n",
    "#         print('path = ', path_to_file)\n",
    "            \n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            im = np.asarray(Image.open(f))/255\n",
    "        f.close()\n",
    "        return im\n",
    "    \n",
    "    \n",
    "    def show_img(self, group_val, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens, then displays image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - group_val should take values in 'train', 'test' or 'val'.\n",
    "        - class_val variable should be set to 'covid' or 'non-covid'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Open image\n",
    "        im = self.open_img(group_val, class_val, index_val)\n",
    "        \n",
    "        # Display\n",
    "        plt.imshow(im)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length special method, returns the number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Length function\n",
    "        return sum(self.dataset_numbers.values())\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Getitem special method.\n",
    "        \n",
    "        Expects an integer value index, between 0 and len(self) - 1.\n",
    "        \n",
    "        Returns the image and its label as a one hot vector, both\n",
    "        in torch tensor format in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get item special method\n",
    "        first_val = int(list(self.dataset_numbers.values())[0])\n",
    "        if index < first_val:\n",
    "            class_val = 'covid'\n",
    "            label = torch.Tensor([1, 0])\n",
    "        else:\n",
    "            class_val = 'non_covid'\n",
    "            index = index - first_val\n",
    "            label = torch.Tensor([0, 1])\n",
    "\n",
    "        im = self.open_img(self.groups, class_val, index)\n",
    "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 train\n",
    "\n",
    "train_group = 'train'\n",
    "l1_ld_train = L1_Lung_Dataset(groups = train_group, \n",
    "                              dataset_numbers = binary_dataset_numbers['layer_1'][train_group], \n",
    "                              dataset_paths = binary_dataset_paths['layer_1'][train_group])\n",
    "l1_ld_train.describe()\n",
    "\n",
    "# You can validate the paths from here\n",
    "# for i in range(3875):\n",
    "#     print(l1_ld_train[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 val\n",
    "\n",
    "val_group = 'val'\n",
    "l1_ld_val = L1_Lung_Dataset(groups = val_group, \n",
    "                              dataset_numbers = binary_dataset_numbers['layer_1'][val_group], \n",
    "                              dataset_paths = binary_dataset_paths['layer_1'][val_group])\n",
    "l1_ld_val.describe()\n",
    "\n",
    "# You can validate the paths from here\n",
    "# for i in range(16):\n",
    "#     print(l1_ld_val[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 test\n",
    "\n",
    "test_group = 'test'\n",
    "l1_ld_test = L1_Lung_Dataset(groups = test_group, \n",
    "                              dataset_numbers = binary_dataset_numbers['layer_1'][test_group], \n",
    "                              dataset_paths = binary_dataset_paths['layer_1'][test_group])\n",
    "l1_ld_test.describe()\n",
    "\n",
    "# You can validate the paths from here\n",
    "# for i in range(380):\n",
    "#     print(l1_ld_test[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_bs_train = 64\n",
    "l1_bs_test = 64\n",
    "l1_bs_val = 1\n",
    "l1_train_loader = DataLoader(l1_ld_train, batch_size = l1_bs_train, shuffle = True)\n",
    "l1_test_loader = DataLoader(l1_ld_test, batch_size = l1_bs_test, shuffle = True)\n",
    "l1_val_loader = DataLoader(l1_ld_val, batch_size = l1_bs_val, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L1_Net, self).__init__()\n",
    "        # Conv2D: 1 input channel, 8 output channels, 3 by 3 kernel, stride of 1.\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, 1)\n",
    "        # change the linear layer to output 3 dim\n",
    "        self.fc1 = nn.Linear(175232, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim = 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate gpu\n",
    "if torch.cuda.is_available():  \n",
    "    print('using GPU')\n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"\n",
    "l1_model = L1_Net().to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(l1_model, (1, 150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criterion and optimizer, epoch\n",
    "epochs = 15\n",
    "lr = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(l1_model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "start_model_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "model_name = 'binary_L1_net'\n",
    "path = './checkpoint_model_{}_{}_{}'.format(epochs, model_name, start_model_time)\n",
    "\n",
    "l1_accuracy_stats_epoch = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "l1_loss_stats_epoch = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    \n",
    "    l1_model.train()\n",
    "    for X_train_batch, y_train_batch in l1_train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = l1_model.forward(X_train_batch)\n",
    "        train_loss  = criterion(output, torch.max(y_train_batch, 1)[1])\n",
    "        train_acc = multi_acc(output, torch.max(y_train_batch, 1)[1])\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "\n",
    "    # testing\n",
    "    with torch.no_grad():\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_acc = 0\n",
    "        l1_model.eval()\n",
    "        for X_test_batch, y_test_batch in l1_test_loader:\n",
    "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "\n",
    "            y_test_pred = l1_model.forward(X_test_batch)\n",
    "\n",
    "            test_loss = criterion(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
    "            test_acc = multi_acc(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
    "\n",
    "            test_epoch_loss += test_loss.item()\n",
    "            test_epoch_acc += test_acc.item()\n",
    "            \n",
    "    \n",
    "    # averaged\n",
    "    train_epoch_loss = train_epoch_loss/len(l1_train_loader)\n",
    "    train_epoch_acc = train_epoch_acc/len(l1_train_loader)\n",
    "    test_epoch_loss = test_epoch_loss/len(l1_test_loader)\n",
    "    test_epoch_acc = test_epoch_acc/len(l1_test_loader)\n",
    "    \n",
    "    # The step number corresponds to the number of batches seen\n",
    "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"Epoch: {}/{} - {} - \".format(e+1, epochs, now),\n",
    "      \"Training Loss: {:.4f} - \".format(train_epoch_loss),\n",
    "      \"Training Accuracy: {:.4f}\".format(train_epoch_acc),\n",
    "      \"Test Loss: {:.4f} - \".format(test_epoch_loss),\n",
    "      \"Test Accuracy: {:.4f}\".format(test_epoch_acc))\n",
    "    l1_model.train()\n",
    "    \n",
    "    # Epoch metrics\n",
    "    l1_loss_stats_epoch['train'].append(train_epoch_loss)\n",
    "    l1_loss_stats_epoch['test'].append(test_epoch_loss)\n",
    "    l1_loss_stats_epoch['epoch'].append(e+1)\n",
    "    l1_accuracy_stats_epoch['train'].append(train_epoch_acc)\n",
    "    l1_accuracy_stats_epoch['test'].append(test_epoch_acc)\n",
    "    l1_accuracy_stats_epoch['epoch'].append(e+1)\n",
    "    \n",
    "    # auto-save a checkpoint at every epoch for evaluation and continual training. Overrides the previous checkpoint\n",
    "    checkpoint = {\n",
    "        'epochs': e+1,\n",
    "        'model_state_dict': l1_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_epoch_loss,\n",
    "        'test_loss': test_epoch_loss, \n",
    "        'test_accuracy': test_epoch_acc,\n",
    "        'training_accuracy': train_epoch_acc\n",
    "    }\n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Training Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(l1_accuracy_stats_epoch, ['train', 'test'], 'accuracy', 'epoch')\n",
    "plot_graph(l1_loss_stats_epoch, ['train', 'test'], 'loss', 'epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "l1_model.eval()\n",
    "for X_val_batch, y_val_batch in l1_val_loader:\n",
    "    X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "    y_val_pred = l1_model.forward(X_val_batch)\n",
    "    \n",
    "    val_acc = multi_acc(y_val_pred, torch.max(y_val_batch, 1)[1])\n",
    "    acc += val_acc.item()\n",
    "\n",
    "print(\"Validation accuracy for layer 1 net\", acc/len(l1_val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinePred(l0_model, l1_model, val_batch):\n",
    "    '''\n",
    "    test_batch will be batch_size of 1 from\n",
    "    '''\n",
    "    l0_model.eval()\n",
    "    l0_pred = l0_model.forward(val_batch)\n",
    "    l0_pred = int(torch.max(l0_pred, 1)[1])\n",
    "    if l0_pred == 0:\n",
    "        # normal\n",
    "        return torch.Tensor([1, 0, 0])\n",
    "    else:\n",
    "        l1_model.eval()\n",
    "        l1_pred = l1_model.forward(val_batch)\n",
    "        l1_pred = int(torch.max(l1_pred, 1)[1])\n",
    "        if l1_pred == 0:\n",
    "            # covid\n",
    "            return torch.Tensor([0, 0, 1])\n",
    "        else:\n",
    "            # non-covid\n",
    "            return torch.Tensor([0, 1, 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "val_loader = DataLoader(ld_val, batch_size = 1, shuffle = False)\n",
    "\n",
    "acc = 0\n",
    "# row = truth, col = pred\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for X_val_batch, y_val_batch in val_loader:\n",
    "    X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "    \n",
    "    y_val_pred = combinePred(l0_model, l1_model, X_val_batch)\n",
    "    y_val_pred = y_val_pred.to(device)\n",
    "    batch_acc = int(torch.argmax(y_val_batch)) == int(torch.argmax(y_val_pred))\n",
    "    y_true.append(int(torch.argmax(y_val_batch)))\n",
    "    y_pred.append(int(torch.argmax(y_val_pred)))\n",
    "    acc += batch_acc\n",
    "    \n",
    "print('combined accuracy:', acc/len(val_loader))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.array(['normal','non-covid','covid']))\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k01aNmdgxVX-"
   },
   "source": [
    "# References\n",
    "\n",
    "Leszczynski, M. (2010). Image Preprocessing for Illumination Invariant Face \n",
    "Verification. Journal of telecommunications and information technology, 19-25."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_dataset_dataloader.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
