{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "custom_dataset_dataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moral-rebound"
      },
      "source": [
        "# Boilerplate notebook"
      ],
      "id": "moral-rebound"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crude-waste"
      },
      "source": [
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# Numpy\n",
        "import numpy as np\n",
        "# Pillow\n",
        "from PIL import Image\n",
        "# Torch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "# Misc\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "id": "crude-waste",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4gBSNDrcfGl"
      },
      "source": [
        "# 1. Download dataset"
      ],
      "id": "R4gBSNDrcfGl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcHbRtuHcd-N"
      },
      "source": [
        "!git clone -b data https://github.com/Oxiang/50.039-Deep-Learning.git"
      ],
      "id": "lcHbRtuHcd-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3ElMDwpU831"
      },
      "source": [
        "!sudo apt-get install tree"
      ],
      "id": "J3ElMDwpU831",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4-h2B3OVCRe"
      },
      "source": [
        "cd 50.039-Deep-Learning"
      ],
      "id": "g4-h2B3OVCRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW2ZD2afqdmF"
      },
      "source": [
        "%%bash\n",
        "\n",
        "(\n",
        "tree dataset -d\n",
        ") "
      ],
      "id": "DW2ZD2afqdmF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWQTRVbAVkPQ"
      },
      "source": [
        "# 2. Dataset high-level info\n",
        "\n",
        "The images stored in the **./dataset** folder and its subfolder consists of 150 by 150 pixels greyscale images, representing X-Ray pictures of lungs.\n",
        "\n",
        "The images, consists of X-ray pictures of the following:\n",
        "\n",
        "| Description                              | Class index | Tensor  | Class label        |\n",
        "| ---------------------------------------- | ----------- | ------- | ------------------ |\n",
        "| People with no infection diagnosis       | 0           | [1 0 0] | normal             |\n",
        "| People with infected lungs and covid     | 1           | [0 1 0] | infected_covid     |\n",
        "| People with infected lungs and non-covid | 2           | [0 0 1] | infected_non_covid |\n",
        "\n",
        "\n"
      ],
      "id": "aWQTRVbAVkPQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZhr10rHVlIj"
      },
      "source": [
        "classes = {0: 'normal', 1: 'infected_non_covid', 2: 'infected_covid'}\n",
        "groups = ['train', 'test', 'val']\n",
        "dataset_numbers = {\n",
        "    'train_normal': 1341,\n",
        "    'train_infected_covid': 1345,\n",
        "    'train_infected_non_covid': 2530,\n",
        "    'val_normal': 8,\n",
        "    'val_infected_covid': 8,   \n",
        "    'val_infected_non_covid': 8,\n",
        "    'test_normal': 234,\n",
        "    'test_infected_covid': 138,\n",
        "    'test_infected_non_covid': 242,\n",
        "}\n",
        "dataset_paths = {\n",
        "    'train_normal': './dataset/train/normal/',\n",
        "    'train_infected_covid': './dataset/train/infected/covid/',\n",
        "    'train_infected_non_covid': './dataset/train/infected/non-covid/',\n",
        "    'val_normal': './dataset/val/normal/',\n",
        "    'val_infected_covid': './dataset/val/infected/covid/', \n",
        "    'val_infected_non_covid': './dataset/val/infected/non-covid/',\n",
        "    'test_normal': './dataset/test/normal/',\n",
        "    'test_infected_covid': './dataset/test/infected/covid/', \n",
        "    'test_infected_non_covid': './dataset/test/infected/non-covid/',\n",
        "}"
      ],
      "id": "NZhr10rHVlIj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kakruEOWFn2"
      },
      "source": [
        "View one of the images and its properties. These images consist of a Numpy array, with values ranging between 0 and 255. These values will be normalized."
      ],
      "id": "_kakruEOWFn2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THPZFpETWAWK"
      },
      "source": [
        "path_to_file = './dataset/train/normal/1.jpg'\n",
        "with open(path_to_file, 'rb') as f:\n",
        "    im = np.asarray(Image.open(f))\n",
        "    plt.imshow(im)\n",
        "f.close()\n",
        "print('Image shape is: {}'.format(im.shape))\n",
        "# Images are defined as a Numpy array of values between 0 and 256\n",
        "print('Image as a numpy array is:\\n {}'.format(im))"
      ],
      "id": "THPZFpETWAWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcs9foiNXKY1"
      },
      "source": [
        "# 3. Creating a Dataset object"
      ],
      "id": "Jcs9foiNXKY1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKLCcIvxXLHu"
      },
      "source": [
        "## 3.1 General Dataset object that is custom made for train, val, test to individually use\n",
        "\n",
        "length method ( __ len __ )\n",
        "\n",
        "> return the number of images present in the dataset\n",
        "\n",
        "getitem method ( __ getitem __ )\n",
        "\n",
        "> fetch an image and its label, using a single index value. Returns the image, along with a one-hot vector corresponding to the class of the object. Both returned parameters will be torch tensors.\n",
        "- [1, 0,0] for normal class\n",
        "- [0, 1, 0] for infected_non_covid class\n",
        "- [0, 0, 1] for infected_covid class"
      ],
      "id": "LKLCcIvxXLHu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtNXQwKbXNfY"
      },
      "source": [
        "class Lung_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Generic Dataset class.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, groups, dataset_numbers, dataset_paths):\n",
        "        \"\"\"\n",
        "        Constructor for generic Dataset class - assembles\n",
        "        the important parameters in attributes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        groups : str\n",
        "            Allowed values: train, val, test\n",
        "        dataset_numbers : dict\n",
        "            Count of each class within specified group\n",
        "        dataset_paths : dict\n",
        "            Path to each class within specified group\n",
        "        \"\"\"\n",
        "\n",
        "        self.img_size = (150, 150)\n",
        "        self.classes = {\n",
        "            0: 'normal',\n",
        "            1: 'infected_covid',\n",
        "            2: 'infected_non_covid'\n",
        "        }        \n",
        "        self.groups = groups\n",
        "        self.dataset_numbers = dataset_numbers\n",
        "        self.dataset_paths = dataset_paths\n",
        "        \n",
        "        \n",
        "    def describe(self):\n",
        "        \"\"\"\n",
        "        Descriptor function.\n",
        "        Will print details about the dataset when called.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Generate description\n",
        "        msg = \"This is the {} dataset of the Lung Dataset\".format(self.groups)\n",
        "        msg += \" used for the Small Project Demo in the 50.039 Deep Learning class\"\n",
        "        msg += \" in March 2021. \\n\"\n",
        "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\n",
        "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
        "        msg += \"The images are stored in the following locations \"\n",
        "        msg += \"and each one contains the following number of images:\\n\"\n",
        "        for key, val in self.dataset_paths.items():\n",
        "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, val, self.dataset_numbers[key])\n",
        "        print(msg)\n",
        "        \n",
        "    \n",
        "    def open_img(self, group_val, class_val, index_val):\n",
        "        \"\"\"\n",
        "        Opens image with specified parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\n",
        "        - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid'.\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
        "        \n",
        "        Returns loaded image as a normalized Numpy array.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Asserts checking for consistency in passed parameters\n",
        "        err_msg = \"Error - group_val variable should be set to 'train', 'test' or 'val'.\"\n",
        "        assert group_val in self.groups, err_msg\n",
        "        \n",
        "        err_msg = \"Error - class_val variable should be set to 'normal' or 'infected_non_covid' or 'infected_covid.\"\n",
        "        assert class_val in self.classes.values(), err_msg\n",
        "        \n",
        "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\n",
        "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
        "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val, max_val)\n",
        "        assert isinstance(index_val, int), err_msg\n",
        "        assert index_val >= 0 and index_val <= max_val, err_msg\n",
        "        \n",
        "        # Open file as before\n",
        "        path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\n",
        "        with open(path_to_file, 'rb') as f:\n",
        "            # Convert to Numpy array and normalize pixel values by dividing by 255.\n",
        "            im = np.asarray(Image.open(f))/255\n",
        "        f.close()\n",
        "        return im\n",
        "    \n",
        "    \n",
        "    def show_img(self, group_val, class_val, index_val):\n",
        "        \"\"\"\n",
        "        Opens, then displays image with specified parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        - group_val should take values in 'train', 'test' or 'val'.\n",
        "        - class_val variable should be set to 'normal' or 'infected'.\n",
        "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Open image\n",
        "        im = self.open_img(group_val, class_val, index_val)\n",
        "        \n",
        "        # Display\n",
        "        plt.imshow(im)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Length special method, returns the number of images in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Length function\n",
        "        return sum(self.dataset_numbers.values())\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Getitem special method.\n",
        "        \n",
        "        Expects an integer value index, between 0 and len(self) - 1.\n",
        "        \n",
        "        Returns the image and its label as a one hot vector, both\n",
        "        in torch tensor format in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Get item special method\n",
        "        first_val = int(list(self.dataset_numbers.values())[0])\n",
        "        second_val = int(list(self.dataset_numbers.values())[1])\n",
        "        if index < first_val:\n",
        "            class_val = 'normal'\n",
        "            label = torch.Tensor([1, 0, 0])\n",
        "        elif index < (first_val+second_val):\n",
        "            class_val = 'infected_covid'\n",
        "            index = index - first_val\n",
        "            label = torch.Tensor([0, 1, 0])\n",
        "        else:\n",
        "            class_val = \"infected_non_covid\"\n",
        "            index = index - (first_val+second_val)\n",
        "            label = torch.Tensor([0, 0, 1])\n",
        "        im = self.open_img(self.groups, class_val, index)\n",
        "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
        "        return im, label"
      ],
      "id": "LtNXQwKbXNfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOYB7aYX9O-"
      },
      "source": [
        "dataset_numbers = {\n",
        "    'train': {\n",
        "        'train_normal': 1341,\n",
        "        'train_infected_covid': 1345,\n",
        "        'train_infected_non_covid': 2530,\n",
        "    },\n",
        "    'val': {\n",
        "        'val_normal': 8,\n",
        "        'val_infected_covid': 8,\n",
        "        'val_infected_non_covid': 8,\n",
        "    },\n",
        "    'test': {\n",
        "        'test_normal': 234,\n",
        "        'test_infected_covid': 138,\n",
        "        'test_infected_non_covid': 242,\n",
        "    }\n",
        "}\n",
        "dataset_paths = {\n",
        "    'train': {\n",
        "        'train_normal': './dataset/train/normal/',\n",
        "        'train_infected_covid': './dataset/train/infected/covid/',\n",
        "        'train_infected_non_covid': './dataset/train/infected/non-covid/',\n",
        "    },\n",
        "    'val': {\n",
        "        'val_normal': './dataset/val/normal/',\n",
        "        'val_infected_covid': './dataset/val/infected/covid/',\n",
        "        'val_infected_non_covid': './dataset/val/infected/non-covid/',\n",
        "    },\n",
        "    'test': {\n",
        "        'test_normal': './dataset/test/normal/',\n",
        "        'test_infected_covid': './dataset/test/infected/covid/',\n",
        "        'test_infected_non_covid': './dataset/test/infected/non-covid/',\n",
        "    }\n",
        "}"
      ],
      "id": "dcOYB7aYX9O-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS4ivSxsYKgl"
      },
      "source": [
        "def verify_dataset(group,dataset,image_overall_index=7,class_val='normal',\n",
        "                   image_specific_dataset_index=1):\n",
        "  print('Verify the special methods __len__ and __get_item__')\n",
        "  print('Number of images in {} dataset: {}'.format(group, len(dataset)))\n",
        "  print('Details for image id {} from the {} dataset'.format(\n",
        "      image_overall_index,\n",
        "      group\n",
        "  ))\n",
        "  im, class_oh = dataset[image_overall_index]\n",
        "  print('Sample image shape: {}'.format(im.shape))\n",
        "  print('Sample image: {}'.format(im))\n",
        "  print('Sample image class: {}'.format(class_oh))\n",
        "\n",
        "  print('\\nVerify the open_img and show_img functions')\n",
        "  print('Open and show image {} from the {}_{} dataset'.format(\n",
        "      image_specific_dataset_index,\n",
        "      group,\n",
        "      class_val\n",
        "  ))\n",
        "  im = dataset.open_img(group, class_val, image_specific_dataset_index)\n",
        "  print('Same sample image shape: {}'.format(im.shape))\n",
        "  print('Same sample image: {}'.format(im))\n",
        "  dataset.show_img(group, class_val, image_specific_dataset_index)"
      ],
      "id": "lS4ivSxsYKgl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9CO29iBYHs-"
      },
      "source": [
        "## 3.2 Train dataset"
      ],
      "id": "s9CO29iBYHs-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBI4uwQqYiWI"
      },
      "source": [
        "train_group = 'train'\n",
        "ld_train = Lung_Dataset(\n",
        "    train_group,\n",
        "    dataset_numbers[train_group],\n",
        "    dataset_paths[train_group]\n",
        ")\n",
        "ld_train.describe()"
      ],
      "id": "LBI4uwQqYiWI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo4ZJTwpYjw4"
      },
      "source": [
        "verify_dataset(train_group,ld_train,1)"
      ],
      "id": "jo4ZJTwpYjw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMz0d7zfY5W7"
      },
      "source": [
        "## 3.3 Validation dataset"
      ],
      "id": "YMz0d7zfY5W7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Sju4KFY5p8"
      },
      "source": [
        "val_group = 'val'\n",
        "ld_val = Lung_Dataset(\n",
        "    val_group,\n",
        "    dataset_numbers[val_group],\n",
        "    dataset_paths[val_group]\n",
        ")\n",
        "ld_val.describe()"
      ],
      "id": "w4Sju4KFY5p8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzhsBRE4Y_F_"
      },
      "source": [
        "verify_dataset(val_group,ld_val,1)"
      ],
      "id": "PzhsBRE4Y_F_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojan7nynZILA"
      },
      "source": [
        "## 3.4 Test dataset"
      ],
      "id": "Ojan7nynZILA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdxD1NzmZIvq"
      },
      "source": [
        "test_group = 'test'\n",
        "ld_test = Lung_Dataset(\n",
        "    test_group,\n",
        "    dataset_numbers[test_group],\n",
        "    dataset_paths[test_group]\n",
        ")\n",
        "ld_test.describe()"
      ],
      "id": "XdxD1NzmZIvq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ohnFncZJJD"
      },
      "source": [
        "verify_dataset(test_group,ld_test,1)"
      ],
      "id": "49ohnFncZJJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XwbFL6DbSxg"
      },
      "source": [
        "# 4. Data visualization\n",
        "\n",
        "This requires a `grouped bar chart`. Refer to [matplotlib Grouped bar chart with labels](https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html) for starter code\n",
        "\n",
        "<u>**Discuss whether or not the dataset is balanced between classes, uniformly distributed, etc.**</u>\n",
        "\n",
        "**Training set**\n",
        "\n",
        "The train data for the different classes are imbalanced. From the graph plotted below, the `infected_non_covid` class has significantly more data points than the other classes. Overall, the ratio `normal:infected_non_covid:infected_covid` is approximately `1:2:1`.\n",
        "\n",
        "This could present more complications if the model is trained in a stacking manner by first training normal vs infected. The ratio of `normal:infected` would be a ratio of `1:3` which is more imbalanced.\n",
        "\n",
        "**Testing set**\n",
        "\n",
        "The test set is also slightly imbalanced with the ratio `normal:infected_non_covid:infected_covid` being approximately `2:2:1`. However, this is not as bad as an imbalanced training set because the test set will not affect the model's parameter tuning.\n",
        "\n",
        "**Validation set**\n",
        "\n",
        "The val set is uniformly distributed between the three classes. However, it is glaring that there are only 8 validation samples for the 3 classes. Considering the ratio of `train:val:test`, the number of validation samples is far too low. For example. with reference to the infected_non_covid class, the ratio of `train:val:test` is `316:1:30`. which is quite far off from the recommended ratios like `80:10:10` or `8:1:1` as indicated by [Stanford's CS230](https://cs230.stanford.edu/blog/split/)."
      ],
      "id": "0XwbFL6DbSxg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-DIZ_jPbVFh"
      },
      "source": [
        "labels = ['normal', 'infected_covid', 'infected_non_covid']\n",
        "\n",
        "train_normal_inc_ic = list(ld_train.dataset_numbers.values())\n",
        "val_normal_inc_ic = list(ld_val.dataset_numbers.values())\n",
        "test_normal_inc_ic = list(ld_test.dataset_numbers.values())\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width = 0.25  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "rects1 = ax.bar(x - width, train_normal_inc_ic, width, label='train')\n",
        "rects2 = ax.bar(x + width, val_normal_inc_ic, width, label='val')\n",
        "rects3 = ax.bar(x, test_normal_inc_ic, width, label='test')\n",
        "\n",
        "ax.set_ylabel('Number of datapoints')\n",
        "ax.set_title('Number of datapoints with respect to each dataset and class')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "id": "x-DIZ_jPbVFh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFl4rsh5p1rX"
      },
      "source": [
        "# 4. Why normalize the data?\n",
        "\n",
        "To recap the normalization can be found in the `Lung_Dataset` class' `open_img` function which did the following normalization.\n",
        "\n",
        "```python\n",
        "# Convert to Numpy array and normalize pixel values by dividing by 255.\n",
        "im = np.asarray(Image.open(f))/255\n",
        "```\n",
        "\n",
        "Images have RGB ranges from 0-255. Considering various activation functions like `sigmoid` such a large range would mean that for vastly different values like 100 and 255, not much difference can be seen when passed into the `sigmoid` activation function. Both would produce a value that is close to 1.\n",
        "\n",
        "Taking the same values as reference, if we divide by 255, for a value of 100,  $\\frac{100}{255}$ we get approximately 0.39. Then for a value of 255, $\\frac{255}{255}$ we get 1. For the initial value of 100 that becomes 0.39 after the division, passing it into `sigmoid(0.39)` produces a value of 0.596. Meanwhile for the initial value of 255 that becomes 1 after division, passing it into `sigmoid(1)` produces a value of 0.731. This difference in value allows us to extract meaningful differences in the pixel values.\n"
      ],
      "id": "BFl4rsh5p1rX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6jfWGpZqCIU"
      },
      "source": [
        "# 5. Other possible pre-processing\n",
        "\n",
        "Form the plot below, which is based on the Training set for normal images as reference, it is evident that there are several differences in the photo dimensions and photo environment. \n",
        "\n",
        "For example, comparing image_index 1 and image_index 28 there is a clear difference in the lighting, Image_index 28 is a lot brighter. One pre-processing step could be to use histogram normalization. There is a paper that recommends 14 possible normalization algorithms that can be performed (Leszczynski, 2010)\n",
        "\n",
        "Aother example is comparing \"skinny\" images like image_index 1 and image_index 31 where there is significantly more dark backgrounds at the side compares to images like image_index 12. Perhaps a edge detection algorithm can be applied to just filter the relevant parts of the image which are the lungs."
      ],
      "id": "d6jfWGpZqCIU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgyLkZ9xp2GH"
      },
      "source": [
        "row = 2\n",
        "col = 2\n",
        "\n",
        "selected_indices = [1,12,28,31]\n",
        "f, axarr = plt.subplots(row,col,figsize=(10,7))\n",
        "counter = 0\n",
        "for row_index in range(row):\n",
        "  for col_index in range(col):\n",
        "    image_index = selected_indices[counter]\n",
        "    im = ld_train.open_img('train', 'normal', image_index)\n",
        "    axarr[row_index,col_index].set_title('Image index: {}'.format(image_index))\n",
        "    axarr[row_index,col_index].imshow(im)\n",
        "    counter += 1"
      ],
      "id": "lgyLkZ9xp2GH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC14yqAz2vSI"
      },
      "source": [
        "# 6. Creating a data loader object"
      ],
      "id": "wC14yqAz2vSI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiftrqf62usb"
      },
      "source": [
        "bs_val = 32\n",
        "train_loader = DataLoader(ld_train, batch_size = bs_val, shuffle = True)\n",
        "test_loader = DataLoader(ld_test, batch_size = bs_val, shuffle = True)\n",
        "val_loader = DataLoader(ld_val, batch_size = 1, shuffle = True)"
      ],
      "id": "uiftrqf62usb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x_-1Qe34U5w"
      },
      "source": [
        "# 7. Model\n",
        "\n"
      ],
      "id": "3x_-1Qe34U5w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yJ7JTk04XIu"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Conv2D: 1 input channel, 4 output channels, 3 by 3 kernel, stride of 1.\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
        "        # change the linear layer to output 3 dim\n",
        "        self.fc1 = nn.Linear(87616, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        output = F.log_softmax(x, dim = 1)\n",
        "        return output"
      ],
      "id": "-yJ7JTk04XIu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJKx7kXTQYQN"
      },
      "source": [
        "# Activate gpu\n",
        "if torch.cuda.is_available():  \n",
        "    print('using GPU')\n",
        "    device = \"cuda:0\" \n",
        "else:  \n",
        "    device = \"cpu\"\n",
        "model = Net().to(torch.device(device))"
      ],
      "id": "EJKx7kXTQYQN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbl4-OO417"
      },
      "source": [
        "summary(model, (1, 150, 150))"
      ],
      "id": "zoBbl4-OO417",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E4MDvTLdnP"
      },
      "source": [
        "# 8. Training the model\n",
        "\n",
        "Reference material: [Towards data science: PyTorch [Tabular] — Multiclass Classification](https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab)"
      ],
      "id": "L1E4MDvTLdnP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98hO2OLpLdRK"
      },
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "    correct_pred = (y_pred_tags == y_test).float()\n",
        "    acc = correct_pred.sum() / len(correct_pred)\n",
        "    \n",
        "    return acc"
      ],
      "id": "98hO2OLpLdRK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7yVj4RlPNNV"
      },
      "source": [
        "# Define criterion and optimizer, epoch\n",
        "epochs = 3\n",
        "lr = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)"
      ],
      "id": "E7yVj4RlPNNV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJxZ00RSPTBy"
      },
      "source": [
        "steps = 0 \n",
        "start = time.time()\n",
        "messages = []\n",
        "\n",
        "accuracy_stats_epoch = {\n",
        "    'train': [],\n",
        "    'test': [],\n",
        "    'epoch': [],\n",
        "    'steps': [],\n",
        "}\n",
        "loss_stats_epoch = {\n",
        "    'train': [],\n",
        "    'test': [],\n",
        "    'epoch': [],\n",
        "    'steps': [],\n",
        "}\n",
        "\n",
        "for e in range(epochs):\n",
        "    train_epoch_loss = 0\n",
        "    train_epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    for X_train_batch, y_train_batch in train_loader:\n",
        "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
        "        \n",
        "        steps += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model.forward(X_train_batch)\n",
        "        train_loss  = criterion(output, torch.max(y_train_batch, 1)[1])\n",
        "        train_acc = multi_acc(output, torch.max(y_train_batch, 1)[1])\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_epoch_loss += train_loss.item()\n",
        "        train_epoch_acc += train_acc.item()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_acc = 0\n",
        "        model.eval()\n",
        "        for X_test_batch, y_test_batch in test_loader:\n",
        "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "\n",
        "            y_test_pred = model.forward(X_test_batch)\n",
        "\n",
        "            test_loss = criterion(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
        "            test_acc = multi_acc(y_test_pred, torch.max(y_test_batch, 1)[1])\n",
        "\n",
        "            test_epoch_loss += test_loss.item()\n",
        "            test_epoch_acc += test_acc.item()\n",
        "    # averaged\n",
        "    train_epoch_loss = train_epoch_loss/len(train_loader)\n",
        "    train_epoch_acc = train_epoch_acc/len(train_loader)\n",
        "    test_epoch_loss = test_epoch_loss/len(test_loader)\n",
        "    test_epoch_acc = test_epoch_acc/len(test_loader)\n",
        "      \n",
        "    # The step number corresponds to the number of batches seen\n",
        "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    msg = \"Epoch: {}/{} - {} - \".format(e+1, epochs, now)\n",
        "    msg += \"Training Loss: {:.4f} - \".format(train_epoch_loss)\n",
        "    msg += \"Training Accuracy: {:.4f} - \".format(train_epoch_acc)\n",
        "    msg += \"Test Loss: {:.4f} - \".format(test_epoch_loss)\n",
        "    msg += \"Test Accuracy: {:.4f}\".format(test_epoch_acc)\n",
        "    messages.append(msg)\n",
        "    print(msg)\n",
        "    model.train()\n",
        "      \n",
        "    # Epoch metrics\n",
        "    loss_stats_epoch['train'].append(train_epoch_loss)\n",
        "    loss_stats_epoch['test'].append(test_epoch_loss)\n",
        "    loss_stats_epoch['epoch'].append(e+1)\n",
        "    accuracy_stats_epoch['train'].append(train_epoch_acc)\n",
        "    accuracy_stats_epoch['test'].append(test_epoch_acc)\n",
        "    accuracy_stats_epoch['epoch'].append(e+1)\n",
        "# autosave model\n",
        "end_model_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "model_name = 'boilerplate_net'\n",
        "checkpoint = {\n",
        "    'c_lr': lr,\n",
        "    'model_name': model_name,\n",
        "    'c_epochs': epochs,\n",
        "}\n",
        "path = './model_{}_{}_{}'.format(epochs, model_name, end_model_time)\n",
        "torch.save(checkpoint, path)"
      ],
      "id": "lJxZ00RSPTBy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqrHiuvOLtAx"
      },
      "source": [
        "mkdir -p results && mkdir -p results/experiments && mkdir -p results/experiments/architecture_selection"
      ],
      "id": "fqrHiuvOLtAx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Bc56UUJVEU"
      },
      "source": [
        "with open('{}'.format('./results/experiments/architecture_selection/multi__net.txt'), 'w') as f:\n",
        "  for msg in messages:\n",
        "      f.write('{}\\n'.format(msg))"
      ],
      "id": "l1Bc56UUJVEU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUG2wrXboT8T"
      },
      "source": [
        "# 9. Graph visualizations of learning curves\n",
        "\n",
        "Graphs covered\n",
        "\n",
        "1. Accuracy against steps\n",
        "2. Recall against steps\n",
        "3. Precision against steps\n",
        "4. F1 against steps\n",
        "5. Loss against steps"
      ],
      "id": "cUG2wrXboT8T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhUVlMGJoW0N"
      },
      "source": [
        "def plot_graph(data,datasets,y_axis,x_axis):\n",
        "  \"\"\"Plots a graph of y against x\n",
        "\n",
        "    Keyword arguments:\n",
        "    data -- dictionary with keys: train, test, epoch\n",
        "    datasets -- list of datasets to plot: train, test\n",
        "    y_axis -- name of the y_axis. example: accuracy\n",
        "    x_axis -- epoch\n",
        "    \"\"\"\n",
        "  plt.figure(figsize=(17,8))\n",
        "  for dataset in datasets:\n",
        "    target_y = data[dataset]\n",
        "    target_x = data[x_axis]\n",
        "    plt.plot(target_x, target_y, label = dataset)\n",
        "  plt.xlabel(x_axis)\n",
        "  plt.ylabel(y_axis)\n",
        "  plt.title('Graph of {} against {}'.format(y_axis, x_axis))\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "id": "JhUVlMGJoW0N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWJD3RFbcyxd"
      },
      "source": [
        "plot_graph(accuracy_stats_epoch, ['train', 'test'], 'accuracy', 'epoch')\n",
        "plot_graph(loss_stats_epoch, ['train', 'test'], 'loss', 'epoch')"
      ],
      "id": "UWJD3RFbcyxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGpqG8Y6es_7"
      },
      "source": [
        "# 10. Validation set performance"
      ],
      "id": "cGpqG8Y6es_7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ku1pIryfPga"
      },
      "source": [
        "def get_ground_truth_prediction(data_loader):\n",
        "    '''Gets the predicted & ground truth labels for each image in data_loader\n",
        "\n",
        "    Keyword arguments:\n",
        "    data_loader -- data loader object\n",
        "    '''\n",
        "    correct = 0\n",
        "    result = {\n",
        "        'im': [],\n",
        "        'ground_truth': [],\n",
        "        'prediction': [],\n",
        "        'index': []\n",
        "    }\n",
        "    for batch_idx, (X_val_batch, ground_truth) in enumerate(data_loader):\n",
        "        ground_truth = torch.max(ground_truth,1)[1].item()\n",
        "        ground_truth = ld_val.classes.get(ground_truth)\n",
        "        prediction = model.forward(X_val_batch)\n",
        "        prediction = torch.max(prediction,1)[1].item()\n",
        "        prediction = ld_val.classes.get(prediction)\n",
        "        if ground_truth == prediction:\n",
        "          correct += 1\n",
        "        im, class_oh = ld_val[batch_idx]\n",
        "        result['im'].append(im)\n",
        "        result['ground_truth'].append(ground_truth)\n",
        "        result['prediction'].append(prediction)\n",
        "        result['index'].append(batch_idx)\n",
        "    return result, correct\n",
        "\n",
        "def plot_ground_truth_prediction(row, col, result):\n",
        "    '''Plots the subplots of images with their ground truth and prediction\n",
        "\n",
        "    Keyword arguments:\n",
        "    row -- integer value for number of rows of plot\n",
        "    col -- integer value for number of columns of plot\n",
        "    result -- dictionary containing index, prediction, ground truth, image\n",
        "    '''\n",
        "    f, axarr = plt.subplots(row,col,figsize=(20,30))\n",
        "    f.suptitle('Validation set pictures, with predicted and ground truth labels.\\nAverage performance {}/{}={}%'.format(correct,total,accuracy))\n",
        "    counter = 0\n",
        "    for row_index in range(row):\n",
        "        for col_index in range(col):\n",
        "            axarr[row_index,col_index].set_title(\n",
        "                'Ground truth label: {}\\nPredicted label: {}'.format(\n",
        "                    result['ground_truth'][counter],\n",
        "                    result['prediction'][counter])\n",
        "                )\n",
        "            axarr[row_index,col_index].imshow(torch.squeeze(result['im'][counter]))\n",
        "            counter += 1\n",
        "\n",
        "val_loader = DataLoader(ld_val, batch_size = 1, shuffle = False)\n",
        "total = len(val_loader)\n",
        "result, correct = get_ground_truth_prediction(val_loader)\n",
        "accuracy = round(correct/total,3) * 100\n",
        "row = 6\n",
        "col = 4\n",
        "\n",
        "plot_ground_truth_prediction(row, col, result)"
      ],
      "id": "8Ku1pIryfPga",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqPhTsfFGgoR"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "val_loader = DataLoader(ld_val, batch_size = 1, shuffle = False)\n",
        "\n",
        "acc = 0\n",
        "# row = truth, col = pred\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for X_val_batch, y_val_batch in val_loader:\n",
        "    X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    y_val_pred = model.forward(X_val_batch.to(device))\n",
        "    y_val_pred = int(torch.max(y_val_pred, 1)[1])\n",
        "    if y_val_pred == 0:\n",
        "        # normal\n",
        "        y_val_pred = torch.Tensor([1, 0, 0])\n",
        "    elif y_val_pred == 1:\n",
        "        y_val_pred = torch.Tensor([0, 1, 0])\n",
        "    else:\n",
        "        y_val_pred = torch.Tensor([0, 0, 1])\n",
        "        \n",
        "    y_val_pred = y_val_pred.to(device)\n",
        "    batch_acc = int(torch.argmax(y_val_batch)) == int(torch.argmax(y_val_pred))\n",
        "    y_true.append(int(torch.argmax(y_val_batch)))\n",
        "    y_pred.append(int(torch.argmax(y_val_pred)))\n",
        "    acc += batch_acc\n",
        "    \n",
        "print('combined accuracy:', acc/len(val_loader))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.array(['normal','covid','non-covid']))\n",
        "disp.plot()"
      ],
      "id": "WqPhTsfFGgoR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDy6THftNqdt"
      },
      "source": [
        "Other useful metrics\n",
        "\n",
        "Precision = $\\frac{tp}{tp + fp}$\n",
        "\n",
        "Recall (also Sensitivity) = $\\frac{tp}{tp + fn}$\n",
        "\n",
        "Specificity = $\\frac{tn}{tn + fp}$\n",
        "\n",
        "For this project, the metrics can be applied differently considering the needs of different stakeholders.\n",
        "\n",
        "**Considering Covid, we cite 2 stakeholders who could have different concerns**\n",
        "\n",
        "For example, the general public could be concerned about the spread of the virus, hence, they could be more worried about Recall because false negatives could mean that people with the virus in the public space and mingling with others.\n",
        "\n",
        "On the other hand, the clinicans could be more concerned about Precision. This is because there are limited hospital beds and it would be a waste of resources to allocated scarce resource to people who do not have covid but were predicted to have covid. This can be a problem if there is a model that predicts everyone to have covid. While this model successfully identifies all covid cases, there will be a lot of people who do not actually have covid being admitted to the hospitals for tratement. This puts a strain on resources.\n",
        "\n",
        "Similarly, they would be interested in high specificity which is the percentage of people who are predicted negative among all the predicitions cases who are truly negative. A higher value for this would similarly reduce the strain of hospital resources as it implies that those who really do not have covid are likely predicted as not having covid and will not need treatment.\n",
        "\n",
        "For the analysis below we focus on `recall` as there was a time when the world was focused on containing the spread of covid which is aligned with the saying that \"prevention is better than cure\". We also output `precision` and the `f1` score to consider that other stakeholders are important as well."
      ],
      "id": "IDy6THftNqdt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEkrCfhxNaJJ"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import json\n",
        "\n",
        "p_r_fs = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0, 1, 2])\n",
        "precision = p_r_fs[0]\n",
        "recall = p_r_fs[1]\n",
        "f_beta_score = p_r_fs[2]\n",
        "\n",
        "overall_results = {}\n",
        "\n",
        "overall_labels = ['normal','covid','non-covid']\n",
        "\n",
        "for index, label in enumerate(overall_labels):\n",
        "  overall_results[label] = {}\n",
        "  overall_results[label]['precision'] = precision[index]\n",
        "  overall_results[label]['recall'] = recall[index]\n",
        "  overall_results[label]['f1_score'] = f_beta_score[index]\n",
        "\n",
        "print(json.dumps(overall_results, indent=4, sort_keys=True))"
      ],
      "id": "EEkrCfhxNaJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k01aNmdgxVX-"
      },
      "source": [
        "# References\n",
        "\n",
        "Leszczynski, M. (2010). Image Preprocessing for Illumination Invariant Face \n",
        "Verification. Journal of telecommunications and information technology, 19-25."
      ],
      "id": "k01aNmdgxVX-"
    }
  ]
}